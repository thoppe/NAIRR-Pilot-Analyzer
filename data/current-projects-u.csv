requestNumber,actionId,actionType,beginDate,endDate,requestID,requestId,requestTitle,pi,piInstitution,piUsername,fos,fosTypeId,opportunity,allocationType,abstract,updateDate
NAIRR240007,161179,New,4/22/24,10/22/24,1256555,1256555,Investigating Security Issues in Instruction-Tuned Large Language Code Models,"Hei, Xiali",University of Louisiana at Lafayette,0000-0002-2438-5430,Other Computer and Information Sciences,502099,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"The incorporation of AI coding tools into software engineering workflows raises significant security concerns. Malicious user prompts or training data attacks may result in AI-generated code containing software backdoors, logic corruption, unauthorized code execution, and other malicious behavior or software exploits. Previous studies have primarily focused on evaluating the robustness of large pre-trained code models like CodeBERT, GraphCodeBERT, and CodeT5 against adversarial examples in tasks such as code clone detection, vulnerability identification, and authorship attribution. However, security issues in advanced and instruction-tuned code LLMs like WizardCoder and CodeLlama have yet to be investigated.

To address this gap, the project aims to systematically uncover novel vulnerabilities and attack vectors introduced by instruction-tuned code LLMs, providing fundamental insights into their implications for cybersecurity and AI-incorporated software engineering workflows. The proposed work represents the first comprehensive investigation into the risks of code injection attacks against instruction-tuned code LLMs. Furthermore, we will develop a technique for identifying adversarial threats against instruction-tuned code LLMs, involving the detection of adversarially manipulated inputs and generated code before execution.",2024-04-26T12:55:13.421Z
NAIRR240008,170056,New,4/22/24,10/22/24,1264626,1264626,"Developing Trustworthy, Robust AI models for Science using Privacy Preserving Federated Learning","Ravi, Madduri",Argonne National Laboratory,nairr-user-xxx,Computer Science,502093,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"Federated learning (FL) is a collaborative learning approach where multiple data owners, referred to as clients, train a model together under the orchestration of a central server by sharing the model trained on their local datasets instead of sharing the data directly. FL enables creation of more robust models without the exposure of local datasets. However, FL by itself, does not guarantee the privacy of data, because the information extracted from the communication of FL algorithms can be accumulated and utilized to infer the private local data used for training. We developed Argonne Privacy Preserving Federated Learning framework (APPFL), with advances in differential privacy, to enable Privacy-Preserving Federated Learning (PPFL). We enabled training of AI models in a distributed setting across multiple institutions, where sensitive data are located, with the ability to scale on supercomputing resources to help create robust, trust-worthy AI models in biomedicine and smart grid applications where data privacy is essential. Setting up a secure federated learning experiment that needs high performance computational resources across distributed sites requires technical capabilities that may not be available for all. To lower the barrier to entry for leveraging PPFL and to enable domain experts in large institutions to utilize FL, we created the Argonne Privacy-Preserving Federated Learning as a service (APPFLx), which enables cross-silo PPFL using easy to use web interface for managing, deploying, analyzing, and visualizing PPFL experiments. 

APPFLx enables secure federations using end-to-end strong Identity and Access Management, where members can create a new federation or join an existing federation using their institutional identities, perform privacy-preserving training on datasets at their respective institutions and securely share the model weights with the service to enable secure aggregation. Existing PPFL frameworks typically involve downloading and configuring complex software, manually creating trust boundaries and identities to enable gradient aggregation, understanding of technical details of underlying deep learning software stack to enable distributed training which is cumbersome. 

APPFLx features include secure distributed training on heterogenous computing resources, choice of over half a dozen federation strategies, including synchronous and asynchronous strategies, integration with TensorBoard capabilities, interfaces to examine data distributions and resource consumption across different sites, detailed reports of different experiments, ability to use model architectures from GitHub or pre-trained models from HuggingFace model repository and the ability to set different hyperparameters of the experiments (like privacy budget to be used in training). 

In this proposal, we aim to create new AI models, fine-tune existing LLMs for biomedical tasks, evaluate various communication, privacy and performance issues inherent in FL.",2024-04-26T14:37:37.922Z
NAIRR240009,170059,New,4/22/24,10/22/24,1264629,1264629,Enhancing Generalization for Detecting AI-Synthesized Fake Multimedia,"Shu, Hu",Purdue University in Indianapolis,nairr-user-xxx,Computer Science,502093,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"DeepFake, a term increasingly mentioned in the news and social media, refers to highly realistic fake images, audios, and videos created using AI algorithms.  By creating illusions of an individual's activities that did not occur in reality, DeepFakes can cause serious harm when they are weaponized. The recent detectors often show good results in intra-domain testing,  where training and testing data are generated using identical forgery techniques. However, real-world scenarios often involve testing data created via unknown forgery methods, differing from the training data. This discrepancy leads to subpar cross-domain detection performance, exemplifying the generalization problem in DeepFake detection. This issue poses a major hurdle for the practical application of existing detection methods. Without such generalization, the current DeepFake detection methods are failing to recognize DeepFakes generated by DeepFake makers using new or evolving techniques and therefore are susceptible to obsolescence easily. This project requests the NAIRR Pilot resources to enhance generalization in detecting novel DeepFakes, aligning with NAIRR's focus area on advancing Safe, Secure and Trustworthy AI and the core research aim of Improving accuracy, validity, and reliability of model performance, while controlling bias. The project's findings will be disseminated through publications in prestigious conferences and journals. All algorithms developed throughout this project will be made openly accessible as open-source software on GitHub.",2024-04-26T14:37:37.922Z
NAIRR240010,170051,New,4/22/24,10/22/24,1264621,1264621,"Training Foundation Models from Private, Federated Client Data","Fanti, Giulia",Carnegie Mellon University,nairr-user-xxx,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"Federated learning (FL) is arguably the most widely-adopted paradigm today for training machine learning models from private, on-device client data. However, with the rise of large language models, many models are too large to fit and train on-device in FL pipelines. Various solutions have been explored, including training sparse subsets of parameters on-device. In this proposal, we study a different paradigm for training foundation models from distributed, private client data: private synthetic data. We aim to study whether distributed clients can help a central server generate private synthetic data that can be used to centrally fine-tune foundation models. We hypothesize that this approach can be used to train private language models much more cheaply than  with FL.",2024-04-26T14:37:37.922Z
NAIRR240011,170055,New,4/22/24,10/22/24,1264625,1264625,Generative Artificial Intelligence and Racial Bias in Disease Representation,"Mankowski, Michal",NYU Langone,nairr-user-xxx,Clinical Medicine,502061,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"This research proposal aims to expand upon the ongoing study on generative Artificial Intelligence (genAI) and racial/sex bias, which investigates the depiction of patients with various diseases by genAI. We will study a broad range of diseases, particularly those with historical stigmatization, and incorporate a wide range of various AI models. The study will examine the sensitivity of genAI to linguistic framing of prompts in the context of different diseases such as cancer, diabetes, syphilis, etc. The research can potentially reveal important insights into understanding and mitigating racial biases and historical stigmatization in AI disease representation.",2024-04-26T14:37:37.922Z
NAIRR240012,170057,New,4/22/24,10/22/24,1264627,1264627,Aligning with Human Preferences for Safe Autonomous Systems,"Sadigh, Dorsa",Stanford University,nairr-user-xxx,Computer Science,502093,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. In this work, we study the problem of learning robot policies and reward functions that are aligned with societal scale objectives and human preferences. This ensures the robot policies learned satisfy desirable safety specifications and can robustly act and interact in human spaces. We plan to leverage and refine large vision-language models that can act as proxy preferences of humans via human feedback. Specifically, we will pretrain and adapt robot policies initialized with these VLMs to achieve the alignment objective. Finally, we will evaluate these policies on real robotic hardware for assistive and home robotics tasks with 7-DoF manipulators.",2024-04-26T14:37:37.922Z
NAIRR240016,170064,New,4/22/24,10/22/24,1264634,1264634,Exploring Backdoor Attacks in Off-the-Shelf Unsupervised Domain Adaptation for Securing Medical Image Analysis,"Liu, Xiaofeng",Massachusetts General Hospital,nairr-user-xxx,"Informatics, Analytics and Information Science",502099,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"Recently, the off-the-shelf model for unsupervised domain adaptation (OSUDA) has been introduced to protect patient data privacy and intellectual property of the source domain without access to the labeled source domain data. Yet, our recent works found that an off-the-shelf (OS) diagnosis model, deliberately compromised by backdoor/Trojan attacks during the source domain training phase, can function as a parasite-host, disseminating the backdoor to the target domain model during the OSUDA stage. Therefore, backdoor attacks can pose a serious threat to the reliability of the target domain medical analysis systems. To our knowledge, this is the pioneer attempt at addressing backdoor attacks in medical image analysis for OSUDA. In this project, we propose to quantify the channel-wise backdoor sensitivity, and eliminate the backdoor infection by overwriting the backdoor-related channels to cut-off the infection. If it is successful, the project will have great potential to achieve a secured, privacy-protected, and cross-center generalizable AI system for medical image analysis.",2024-04-26T14:37:37.922Z
NAIRR240020,167306,New,4/22/24,10/22/24,1262279,1262279,Energy-based Robust and Safe Reinforcement Learning Via Diffusion Models,"Chen, Haipeng",William & Mary,0000-0003-0572-8888,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"This proposal aims to improve the existing MaxEnt Reinforcement Learning algorithm S2AC (Messaoud et al., 2024) by replacing the SVGD policy with a Diffusion Model-based policy. Since Diffusion Models are a class of generative models that are practically more stable than SVGD, we conjecture that the performance, especially robustness and safety of S2AC would be significantly enhanced by replacing the SVGD policy with Diffusion Model.",2024-05-02T18:06:23.053Z
NAIRR240021,167470,New,4/22/24,10/22/24,1262408,1262408,Determining an optimal initial fluid rescucitation regime for patients with sepsis using machine learning,"Shteyler, Vadim","University of California, San Francisco",nairr-user-43gxm,Clinical Medicine,502061,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"I am a postdoctoral trainee on a TL1 grant doing research and learning machine learning tools for causal inference. This research aims to reduce mortality in patients with sepsis, a heterogenous life-threatening response to infection characterized by organ dysfunction and immune dysregulation; it impacts millions of people annually worldwide and has a 15-30% mortality. Early intravenous fluid administration and vasopressors remain the hallmarks of sepsis-associated hypotension and shock treatment. However, high-quality evidence for any specific initial fluid resuscitation strategy is lacking. Nonetheless, Surviving Sepsis Campaign (SSC) guidelines and the Centers for Medicare and Medicaid Services (CMS) Severe Sepsis/Septic Shock Early Management Bundle (SEP-1) recommend an initial 30 cc/Kg ideal body weight (IBW) fluid administration within 3 hours of diagnosis and, if hypotension persists, vasopressor initiation within 6 hours of diagnosis. To determine whether this approach is, indeed, beneficial for patients with sepsis, we propose to conduct this retrospective observational cohort study to estimate the average fluid volume and administration rate that is associated with the lowest hospital mortality for most patients, using Targeted Maximum Likelihood Estimation (TMLE) and Longitudinal TMLE (LTMLE) with the ensemble machine learner SuperLearner (SL).",2024-04-26T13:28:48.385Z
NAIRR240023,170061,New,4/22/24,10/22/24,1264631,1264631,Interpretable Bayesian Deep Learning: From Neural Networks to Large Language Models,"Wang, Hao",Rutgers University - New Brunswick,nairr-user-xxx,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"Interpretability is one of the fundamental obstacles hindering the adoption and deployment of deep learning (DL) systems, from typical deep neural nets like CNNs/RNNs to large language models (LLMs) and large multimodal models (LMMs) like GPT-4(V), across various fields such as healthcare, e-commerce, and manufacturing. An ideal interpretable model should be able to produce symbolic representations understandable by humans (e.g., what diseases and symptoms each variable in the model represents), conform to conditional dependencies in the real world (e.g., whether the customer's purchase is affected by the product's price drop), and handle uncertainty in data (e.g., how certain the model is about the rainfall tomorrow). Unfortunately, DL as a connectionist approach does not natively support any of these desiderata.

This project aims to develop two sets of methodologies grounded in Bayesian deep learning: (1) ""Bayesian Deep Interpreters,"" which will interpret deep learning models through the use of graphical models that describe the conditional dependencies leading to current predictions. (2) ""Bayesian Deep Controllers,"" which will manage the predictions of deep learning models by adjusting specific random variables within the graphical models associated with the controlled models.",2024-04-26T14:37:37.922Z
NAIRR240024,170058,New,4/22/24,10/22/24,1264628,1264628,Evaluating and Mitigating Bias in Autonomous Patient Monitoring from ICU Videos,"Schulman, Kevin","Clinical Excellence Research Center (CERC), Stanford School of Medicine",nairr-user-xxx,Health Sciences,502061,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"As part of our research objective to advance ambient intelligence in healthcare, we propose developing computer vision models to predict Richmond Agitation-Sedation Scale (RASS) scores from video streams of cameras in ICU patient rooms. RASS score is a critical indicator of a patient's level of consciousness and agitation. By accurately predicting RASS scores in real time, we can help clinicians manage patient sedation, improve patient outcomes, and enhance the quality of care provided. Importantly, our model development process incorporates robust measures to mitigate bias related to race and demographics, ensuring equitable predictions across diverse patient populations.",2024-04-26T14:37:37.922Z
NAIRR240027,170050,New,4/22/24,10/22/24,1264620,1264620,Enhancing Scalability and Privacy of Federated LLMs,"Ding, Yufei","University of California, San Diego",nairr-user-xxx,Computer Science,502093,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"This proposal explores the evolving landscape of Large Language Models (LLMs) like GPT, LLaMa, and PaLM, which have revolutionized areas including ChatBot interaction, time series forecasting, and code generation. Central to LLMs' effectiveness is the scale of their architecture and the extent of training data. However, the reliance on extensive, diverse datasets challenges the capacity of public data, necessitating the integration of private domain data. This integration, while promising, is hindered by privacy and competitive concerns. To navigate these challenges, the proposal emphasizes Federated Learning (FL) for LLMs. FL enables decentralized training across multiple private domains without relocating the data, addressing privacy issues. Yet, FL introduces systemic challenges, such as hardware heterogeneity and connectivity variances, which can compromise training effectiveness. Additionally, privacy concerns in FL are amplified in LLMs due to the influential role of fewer participants, raising the risk of peer-to-peer data breaches. The research focuses on two key areas: developing a Scalable Federated Learning System for LLMs and formulating a Security and Privacy Preserving Algorithm for their training. This approach aims to refine federated training processes and algorithms, setting a benchmark in federated LLM training. The anticipated outcome is a versatile, secure federated learning framework applicable across various domains, demonstrating the extensive potential of FL in advancing LLMs.",2024-04-26T14:37:37.922Z
NAIRR240028,170065,New,4/22/24,10/22/24,1264635,1264635,Build Reliable and Secure AI Surrogates for Large-Scale Scientific Applications with Portability Performance Analysis,"Dong, Wenqian",Florida International University,nairr-user-xxx,Applied Computer Science,502099,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"Large-scale scientific simulations drive scientific and engineering discovery across many domains, but face performance problems. These simulations typically involve complex physics computations, such as (non-)linear programming problems in transportation, manufacturing, robotics, power grids, and renewable energy systems, which causes unaffordable running time. However, the computation components in the simulations are difficult to scale efficiently on high-performance hardware. 

In the next decade, Artificial Intelligence (AI) and Machine Learning (ML) may revolutionize the natural sciences, enhancing our capacity to model and predict natural occurrences. This could herald a new era of scientific exploration, bringing significant advancements across sectors from drug development to renewable energy.  By employing reverse-engineering and automatic learning methodologies it is often possible to solve complex, unstructured problems with a fraction of the computing power and execution time needed by traditional direct and first-principle methods. A deep neural network provides researchers with a powerful tool to learn the structure of physical phenomena directly from Nature, rather than having to explain the causal relationships through the direct application of physics law. Researchers have found that using AI techniques to act as surrogates for the computation-intensive parts of High-performance Computing (HPC) programs can achieve excellent accelerations.",2024-04-26T14:37:37.922Z
NAIRR240030,169422,New,4/22/24,10/22/24,1263963,1263963,Improving Factuality in Automated Course Logistics Question Answering,"Lan, Andrew",University of Massachusetts Amherst,0000-0002-8475-6600,Educational Sciences,502072,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We have collected a dataset, which we call SyllabusQA, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Our goal is to benchmark several strong baselines on this task, from large language model (LLM) prompting to retrieval-augmented generation (RAG), to investigate whether existing methods can perform well in terms of both textual similarity and fact precision.",2024-04-30T14:20:14.371Z
NAIRR240031,170048,New,4/22/24,10/22/24,1264618,1264618,"Advanced Training for Protein Diffusion, Binder Prediction, and Antibody Design","Baker, David",University of Washington Howard Hughes Medical Institute,nairr-user-xxx,Biochemistry and Molecular Biology,502061,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"Advances in deep learning have enabled accurate protein structure prediction and are massively speeding up biochemical discoveries. We recently developed RoseTTAFold All-Atom and RFDiffusion, allowing prediction and generation of complicated biomolecules, including non-amino acids. While seeing early success, current models encounter challenges for designing binders to polar flexible ligands, antibody-protein interactions, and enzyme catalytic sites. We propose to fine-tuning our current models for specific problems, such as antibodies and polar ligands, to quickly improve the model performance even with limited training resources and enable us to tackle problems currently inaccessible but more clinically related.",2024-04-26T14:37:37.922Z
NAIRR240032,170060,New,4/22/24,10/22/24,1264630,1264630,Developing a benchmark hydrologic dataset and a fast AI surrogate model for assessing climate impacts on mountainous hillslopes,"Wang, Lijing",Lawrence Berkeley National Laboratory,nairr-user-xxx,Hydrology and Water Resources,502065,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"Mountainous watersheds serve as the water tower for downstream rivers. However, observations of discharge and groundwater levels are limited at the subcatchment level, and using hydrologic modeling directly at the subcatchment scale without observations will not provide realistic predictions. To address this sparse data gap, this project aims to simulate stochastic hydrologic responses across 2,000 mountainous hillslopes under various climate scenarios and uncertain subsurface and surface properties. The baseline mountainous hillslope model we used has integrated intensive geophysical and hydrologic datasets. The simulated 2,000 hydrologic responses become an AI-ready dataset to benchmark realistic hydrologic variations. Additionally, leveraging the Neural Operator, we plan to develop a fast AI surrogate model to quickly predict snowpack, discharge, and groundwater levels, substituting time-intensive hydrologic simulations. This pilot study not only advances hillslope hydrologic models by incorporating realistic subsurface and surface uncertainties but also contributes an openly accessible hydrologic dataset and a pretrained AI model for rapid hydrologic forecasting in mountainous regions, supporting water budgeting and climate impact assessments.",2024-04-26T14:37:37.922Z
NAIRR240038,170049,New,4/22/24,10/22/24,1264619,1264619,Towards Mechanistic Audits of LLMs,"Crovella, Mark",Boston University,nairr-user-xxx,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"The safety of machine learning tools is a critical societal concern.  The imment wide deployment of LLMs in society elevates the need for principled methods for auditing LLMs for socially undesirable behavior.   While previous attempts to perform algorithmic audits are often black-box, we propose to explore the white-box auditing of open source LLMs via the techniques of mechanistic interpretability.   This project will develop a large test suite of datasets corresponding to socially relevant concepts.  We will then use these datasets to probe a suite of LLMs to determine for which concepts and models the linear representation hypothesis holds.  In those cases, we will examine the resulting linear representations for evidence of undesirable interaction, for example, through interference due to superposition.  All datasets and results will be published in the open literature.",2024-04-26T14:37:37.922Z
NAIRR240040,170062,New,4/22/24,10/22/24,1264632,1264632,The Development Safety of Large Foundation Models,"Yang, Tianbao",Texas A&M University,nairr-user-xxx,Computer Science,502093,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"This project will investigate an importance issue of continual developing large foundation models. While large foundation models have demonstrated remarkable capabilities across various tasks, they are not without limitations, often necessitating iterative updates to enhance their performance in specific domains or tasks. In order to enhance the system's ability to adapt to the changing and complex world, it is necessary to undergo multiple iterations of model development, which involve collecting new data and training new
models. However, this iterative model development process raises significant safety issues that have been overlooked by existing studies in AI, i.e., the model development for acquiring new safety features may inadvertently lose the previously ensured safety features of the old model. This project will investigate novel approaches to address these issues.",2024-04-26T14:37:37.922Z
NAIRR240042,170047,New,4/22/24,10/22/24,1264617,1264617,Reinforcement Learning from Conversational Signals,"Artzi, Yoav",Cornell University,nairr-user-xxx,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"We propose to fine-tune language models from interaction with users without explicit feedback, but instead from signals arising from how humans respond to system outputs in multi-turn conversational interactions. Unlike explicit human feedback, these signals require no annotation effort beyond the interaction itself. We develop an approach to learn from human response to system utterances to improve an underlying large language  model. We use two interaction scenarios for our studies and experiments, and evaluate through deployment with human users.",2024-04-26T14:37:37.922Z
NAIRR240044,169726,New,4/22/24,10/22/24,1264312,1264312,Training and Benchmarking Three Different Foundation Models on Public Histopathology Gigapixel Images for Whole Slide Image Representation,"Tizhoosh, Hamid",Mayo Clinic ,0000-0001-5488-601X,Performance Evaluation and Benchmarking,502085,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"This proposal addresses the challenge of transforming whole slide images (WSIs) into actionable vectors for different computational pathology (CP) tasks. Despite recent progress in leveraging artificial intelligence (AI) in CP, WSIs pose unique challenges due to their large size and complex features. Our objective is to secure resources to train and benchmark three foundation models tailored for WSI representation. We aim to utilize publicly available datasets to conduct the largest-ever effort in training and benchmarking models at the WSI level. The selected models include LongNet, LambdaNetworks, and RWKV, each offering distinct approaches to long sequence analysis. Leveraging diverse datasets such as TCGA, PANDA, CAMELYON16, BRACS, and BCNB, we will ensure comprehensive coverage for robust training and evaluation. The evaluation will assess the performance of each model across relevant metrics, providing standardized findings for comparison. Given the massive size of WSIs, substantial computational and storage resources are required. Successful implementation of this proposal will advance research in CP, driving innovation and clinical translation. By providing comprehensive evaluations, this initiative empowers the research community to make informed decisions, ultimately improving patient outcomes.",2024-04-26T14:10:29.952Z
NAIRR240045,170052,New,4/22/24,10/22/24,1264622,1264622,"Guardians of Integrity in AI: Establishing Trust, Originality, and Ethical Standards (GATES)","Huang, Furong",University of Maryland,nairr-user-xxx,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"The advent of generative AI has ushered in unprecedented capabilities alongside significant ethical and security challenges. Guardians of Integrity in AI: Establishing Trust, Originality, and Ethical Standards (GATES) proposes an innovative research agenda aimed at fortifying the ethical backbone of artificial intelligence. This project targets three pivotal areas: the detection of AI-generated content to combat misinformation, the protection of intellectual property rights to uphold the originality and fairness in AI-generated works, and the development of trustworthy foundation models for enhanced human-AI collaboration. Through GATES, we intend to develop robust methodologies and frameworks that ensure AI systems are safe, secure, and aligned with human values. This initiative not only addresses immediate concerns within the AI community but also lays the groundwork for sustainable, ethical AI development, resonating with the goals of the NAIRR Pilot initiative. By integrating advanced computational techniques with ethical principles, GATES aims to lead the way in establishing a new standard for trustworthy AI, ensuring that these powerful technologies augment human capabilities without compromising ethical standards.",2024-04-26T14:37:37.922Z
NAIRR240046,169739,New,4/22/24,10/22/24,1264321,1264321,Enhancing LM Adaptation toward Safe and Trustworthy AI,"Hajishirzi, Hanna",University of Washington,0000-0002-1055-6657,Computer Science,502093,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"The capabilities of large language models (LMs) to follow user requests have been progressing rapidly through a wide range of openly available models, datasets, and training methods. In the meantime, we need to make these models safe, secure, and trustworthy. There has been rapid progress in both improving capabilities and safety of language models. Our team at UW are at the forefront of building open-source language models and adapting them to new capabilities and safety measures~\citep{wang2023far}, and making them trustworthy~\cite{asai-etal-2023-retrieval}. Since the release of our state-of-the-art models~\citep{wang2023far},  there have been a number of significant advances in almost all aspects of  language model adaptation, from the release of improved finetuning datasets~\citep{UltraChat, cui2023ultrafeedback}, to  increasingly powerful base models~\citep{touvron2023llama,jiang2023mistral}, to powerful and accessible adaptation methods for combining these components~\citep{rafailov2023direct, dettmers2023qlora}, and retrieval-augmentation models~\cite{asai-etal-2023-retrieval}. Building on our team's prior expertise, we propose to introduce and investigate approaches to make language models safe  and trustworthy.",2024-04-15T20:05:49.132Z
NAIRR240051,170054,New,4/22/24,10/22/24,1264624,1264624,Neuro-inspired Oversight for Safe and Trustworthy Large Language Models,"Kim, Edward",Drexel University,nairr-user-xxx,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"We have observed that improving instruction following is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines.  We have previously showed that larger parameter models exhibit superior capability in following instructions that require overriding both internal knowledge and contextual cues, demonstrating a high degree of obedience.  However, our research also highlights a fundamental issue between enhancing a model's ability to override instructions and maintaining adherence to safety protocols and guidelines.  Specifically, the most responsive instruction following LLMs are the ones that can be most easily jailbroken.

In our work, we believe that the architecture of the human brain illustrates a critical point, there is a separation between language understanding and moderation.  This is supported by functional mapping studies that show the language understanding center, e.g. Wernicke's area, and speech moderation and production area, Broca's area and surrounding Frontal Lobe, are in two distant and separate areas of the brain.  We are pursuing research in LLM moderation through the lens of biological intelligence.",2024-04-26T14:37:37.922Z
NAIRR240052,170063,New,4/22/24,10/22/24,1264633,1264633,"Pre-training a generative selective state space model, the Mamba model, on UCSF-specific deidentified clinical notes and time-series structured data","Sushil, Madhumita","University of California, San Francisco",nairr-user-xxx,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"While large language models (LLMs) have made unprecedented advances in the general domain, their advances in medicine are limited by access to large datasets and advanced computing infrastructure. Existing publicly available LLMs are not always sufficient for healthcare-specific use, since they either lack domain knowledge, are expensive to use, or cannot be accessed in a HIPAA-compliant setting. To enable more widespread use of AI within medicine, we aim to develop an effective and task-independent foundation model for medicine trained on a large and diverse corpus of deidentified patient data from electronic health records at University of California San Francisco. A recent family of models, selective state space models, have demonstrated promising capabilities in long sequence processing, providing competitive performance to transformers-based models. In this study, we aim to develop a longitudinal selective state space LLM on medical data by encoding sequential knowledge from clinical encounters for generating future clinical states and actions.",2024-04-26T14:37:37.922Z
NAIRR240055,170053,New,4/22/24,10/22/24,1264623,1264623,Securing Healthcare Privacy: Rendering Large-Scale Unlearnable Medical Imaging Data to Prevent Data Leaks,"Huo, Yuankai",Vanderbilt University,nairr-user-xxx,Computer Science,502093,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"This proposal outlines an innovative secure and trustworthy AI approach to safeguard healthcare data from unauthorized use in AI model training through the development of unlearnable examples (UE). As AI models, including foundation models like ChatGPT, SAM, and Sora, increasingly raise privacy and safety concerns, especially with the unauthorized use of patient data, the National Artificial Intelligence Research Resource (NAIRR) project aims to pioneer privacy-ensured radiological AI. By embedding invisible noise in over 2 million radiological images from MRI and CT scans, this project will generate label-agnostic UE models that protect medical imaging data against misuse in future AI training, without compromising their clinical utility. The project will leverage cutting-edge UE algorithms, high-performance computing, and Vision Transformer models to achieve its objectives. It addresses the urgent need for robust privacy measures in healthcare data, responding to the risks posed by the repurposing of medical imaging for AI training without explicit consent. The proposal also outlines a comprehensive strategy to overcome the challenges of developing UE models for healthcare, including the need for large-scale medical image datasets and substantial computational resources. Aim 1 focuses on rendering large-scale medical imaging data unlearnable, while Aim 2 assesses the effectiveness of UE models against various target models and advanced defenses. The project plans a six-month timeline to achieve its goals, aiming for outcomes that will be shared in leading academic venues. The success of this NAIRR project is expected to significantly advance the field of AI healthcare data security by utilizing Vanderbilt's clinical imaging data in conjunction with DOE's Leadership Computing Facility. This will lay the groundwork for future collaborations and open new avenues for healthcare and high-performance computing (HPC) AI research, ensuring privacy and intellectual property protections for sensitive healthcare data.",2024-04-26T14:37:37.922Z
NAIRR240057,169780,New,4/22/24,10/22/24,1264357,1264357,Unified Representation Learning,"Shrivastava, Abhinav","University of Maryland, College Park",0000-0001-8928-8554,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"Deep learning has dominated the fields of generative modeling and discriminative visual recognition for the past decade. For generative tasks, a deep learning model seeks to synthesize or edit parts of images, while for discriminative tasks, it learns to label images or parts of images.  We argue that both of these can be considered complementary and intuitively should help each other because both need a semantic understanding of the underlying structure. We will develop unsupervised unified representations, for generative and discriminative tasks, to be used as general-purpose representations for various downstream tasks like image recognition, reconstruction, and synthesis. Such unified models can be efficiently finetuned for multiple downstream tasks, as opposed to having to pre-train large, expensive models separately for different tasks.",2024-04-15T20:35:30.212Z
NAIRR240088,169923,New,4/22/24,10/22/24,1264511,1264511,Use of AI and sub-meter resolution satellite imagery to map permafrost thaw disturbances and human-built infrastructure at pan-Arctic scale,"Witharana, Chandi","University of Connecticut (Storrs, CT)",nairr-user-agqmx,Geology and Solid Earth Sciences,502117,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"The Arctic is experiencing more rapid warming than many other regions leading to degradation of permafrost, - the permanently frozen earth materials that remain at 0Â°C or below for two or more consecutive years-, at an alarming rate. Widespread permafrost thaw induced disturbances negatively impact an array of natural processes, altering hydrology, vegetation dynamics and biogeochemical fluxes, and posing serious threat to human-built environment by damaging buildings and critical infrastructure sitting on permafrost-affected ground.  Permafrost thaw disturbances and microtopographic transitions are no longer localized phenomena but extend across the Arctic. Conventional field surveys and local-scale monitoring mechanisms are unable to offer synoptic assessments of permafrost degradations and ramifications on natural and human-built environments.  Consequently, there is a dire need for new monitoring tools to precisely map permafrost landforms and thaw disturbances and monitor their changes over time and assess the permafrost thaw risk on human-built infrastructure. 
Permafrost dominated landscape in the Arctic harbor a variety of distinctive landforms shaped by the freezing and thawing of the ground.  Ice-wedge polygons (IWPs), the most conspicuous and critical microtopographic formation found in the cold continuous permafrost regions, are underlain by several meter-wide and deep ice-wedges that form a network across the tundra. The microtopography associated with IWPs dictates the Arctic ecosystem from local to regional scales due to the impacts on the flow and storage of water and consequently alter vegetation and carbon dynamics. Field and remote sensing-based studies reported ice-wedge degradation transformation of low-centered polygons into high-centered polygons due to permafrost thaw at several locations across the Arctic tundra. Spatially patchy and temporally sparse knowledge on ice-wedge polygon system's dynamics inevitably lead to uncertainties in regional and pan-Arctic estimates of carbon, water, and energy fluxes. Closing such critical information gaps and improving multi-scale models require objective and detailed geospatial data sets consolidating the ice wedge polygon extent and their prevailing successional stages.
The entire Arctic has been imaged by sub-meter resolution Maxar satellite sensors multiple times during the last two decades. Data repositories holding millions of Maxar images are now at petabyte scale. However, comprehensive Pan-Arctic science products derived from this imagery are scarce. This is largely due to data size (big data) and semantic complexity of satellite images. With the funding from NSF Polar Programs (award #s: 1927723, 1827872, 1927720), we pursue the first-of-its-kind permafrost science use case that capitalizes on a large volume of commercial satellite imagery to create the first Pan-Artic scale ice-wedge polygon map covering > 5 million km2 in the Arctic. We harnessed thousands of Maxar imagery, deep learning CNNs, and high-performance computing resources to develop a scalable and reproducible image-to-assessment pipeline. The operational implementation of our mapping pipeline on high-performance computing systems has detected more than one billion individual IWPs. The deep learning model predictions comprise spatial outlining of IWPs coupled with their classification and geometrical attributes at individual IWP-level. The resulting Pan-Arctic ice-wedge polygon map holds significant potential to serve a diverse user community, facilitating a deeper understanding of the intricate and interconnected processes governing the evolution of the ice-wedge polygon tundra landscape. 
The technical goal of this proposal is to improve the performance of our GeoAI pipeline by integrating vision transformers and extending its capabilities to map other important features. Self-attention-based neural net architectures, in particular Transformers have shown extraordinary success in multiple domains, especially in natural language processing (e.g., Chat GPT). Since foundation models are trained on internet-scale training data sets, they can be generalized with zero-shot and few-shot learning to a new task. The introduction of Segment Anything Model (SAM) is an attempt to come out with a foundation model for image segmentation tasks. SAM enables image segmentation without having to train the model on a large set of training samples (zero-shot learning). As with ChatGPT, SAM also uses a transformer-based approach. DL is rapidly moving away from the CNN-based computer vision models to vision transformers (ViTs). Currently, ViT-based models exhibit great promise on benchmark segmentation datasets. However, one major challenge is in adopting ViTs from everyday images to be able to segment geo-objects in high resolution satellite images. Continued access to HPC resources is critical for us to improve the existing DL pipelines via the introduction of new technologies, training and tuning of models, and to create new geospatial map products and reach our new aims successfully. The central objective of this proposal is to request HPC resources that are necessary to develop, train and validate transformer-based AI models.",2024-04-26T14:37:37.922Z
NAIRR240091,169428,New,4/22/24,10/22/24,1263968,1263968,Privacy-Preserving Synthetic Data Generation for AI Research,"De Cock, Martine",University of Washington Tacoma,0000-0001-7917-0771,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"Data is the lifeblood of AI. Much of the most valuable data in the nation however is siloed in research centers, hospitals, banks, etc. The long and onerous processes that researchers have to go through to access each silo is causing a substantial underutilization of AI in many of the most important domains, including healthcare, genomics, justice, education, and finance. Synthetic data generation (SDG) offers an appealing solution to make data more broadly available for AI research while mitigating privacy concerns. Current SDG algorithms however lack provisions for protection of input privacy. Our goal is to leverage the NAIRR infrastructure to make it possible for data holders to contribute their data to an SDG process without disclosing that data in an unencrypted manner. The enhanced privacy brought by the ability to train synthetic data generators over encrypted data comes at a significant computational cost, which is a justifiable price to pay in very sensitive domains like healthcare and genomics.",2024-04-26T14:56:11.589Z
NAIRR240113,169997,New,4/22/24,10/22/24,1264568,1264568,"Scalable Editing of Vision-Language Models, With Applications to Cyber-Physical Systems","Hegde, Chinmay",New York University,0000-0003-4574-8066,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"This NAIRR Pilot project seeks to enable significant advances in scalable model editing approaches for making multimodal AI models safe, robust, and aligned with desirable societal values. Model editing refers to the process of systematically adjusting the weights of an existing (trained) AI model so that the new model obeys certain desiderata. Most model editing approaches suffer from issues of scale (they work only for small-scale models) and extent (they are only able to change model properties to a limited degree). Our project seeks to test novel strategies that can significantly surpass such limitations.",2024-04-16T14:26:27.624Z
NAIRR240115,170002,New,4/22/24,10/22/24,1264573,1264573,Agricultural pest identification with robustness guarantees,"Ganapathysubramanian, Baskar",Iowa State University,0000-0002-8931-4852,"Agriculture, Forestry, and Fisheries",502065,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"This proposal requests computational resources on TACC Frontera to advance a transformative project in agriculture: developing a robust foundational vision model for pest identification (insects, weeds, and diseases). It builds on preliminary work with InsectNet, a model trained on a large dataset of insect images, to expand its capabilities significantly. The team aims to utilize an expanded dataset of approximately 100 million images and incorporate new architectures, including transformers, to enhance model accuracy, scalability, and robustness. A key innovation is the introduction of certified robustness to ensure model reliability against adversarial attacks, making it a crucial tool for improving agricultural outcomes globally. This project is supported by the USDA-NIFA and NSF-funded AI Institute for Resilient Agriculture and an NSF CPS Frontiers grant, underscoring its importance and potential impact on sustainable food production.",2024-04-15T20:39:07.485Z
NAIRR240122,170005,New,4/22/24,10/22/24,1264576,1264576,Community Runnable Earth Digital Intelligence Twin Training and Reforecats,"Gagne, David",National Center for Atmospheric Research,0000-0002-0469-2740,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"The goal of the Community Runnable Earth Digital Intelligence Twin (CREDIT) project is to develop, train, and evaluate global and regional artificial intelligence numerical weather prediction (AI NWP) models in an open, scaleable, science-focused framework. We will investigate how choices in the data processing, architecture, loss functions, and evaluation lead to more accurate, stable, physically consistent, and valuable AI NWP models and produce large sets of reforecasts for further analysis and evaluation by the broader community.",2024-04-16T13:56:40.979Z
NAIRR240129,169898,New,4/22/24,10/22/24,1264497,1264497,Exploring Iterative Reasoning in LLMs and the Transferability to Multimodal Models,"Winder, John",Johns Hopkins University Applied Physics Laboratory,nairr-user-ikier,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"This project aims to explore the reasoning process of Large Language Models (LLMs) and investigate the generalizability of these capabilities from text to multimodal contexts using joint embedding models. We hypothesize that transformers, the underlying architecture of LLMs, perform semantic reasoning through an iterative numerical optimization process. Through analyzing token transformations and employing techniques like principal component analysis (PCA), we seek to understand and interpret LLM's reasoning processes. Additionally, we will investigate the transferability of this reasoning process to other modalities by leveraging the ImageBind model. This study not only seeks to better understand the reasoning mechanisms of LLMs but also to apply these findings to modalities beyond text.",2024-04-16T13:43:48.875Z
NAIRR240140,170018,New,4/22/24,10/22/24,1264589,1264589,Developing Reliable Agents for Automating Complex Digital Tasks,"Balasubramanian, Niranjan",Stony Brook University,0000-0003-4187-9368,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"Generative AI has greatly expanded the scope of automation in complex tasks. Many day-to-day digital tasks require interacting with tools via APIs to solve problems in a step-by-step manner. For example, paying what we owe friends for a night out involves using apps such as Splitwise, emails, and Venmo. Many such common activities are complex tasks, requiring our interaction with multiple applications. If LLMs can be augmented with API-based access to these apps, we can automate such complex tasks. There have been significant advances in LLM tool use, and an increased push for automation in solving complex tasks. ChatGPT Plugins, for example, aim to support real-time information retrieval, access to specific knowledge bases, and perform actions such as booking a flight on behalf of a user. There is vast potential for solving a wide variety of complex tasks through such LLM-based automation. However, translating this potential into viable and verified real-world solutions requires both substantial advances in model capabilities and rigorous evaluation in a controllable environment. To this end, this team has already developed, AppWorld, an execution environment, a collection of complex tasks, and programmatic evaluation to verify the correctness of the solutions exhaustively. Benchmarking results show that standard ways of applying state-of-the-art models are unreliable and ineffective for these tasks. This proposal is aimed at developing and training reliable LLM-based agents using three approaches: (i) Supervising LLMs with iterative bootstrapping of tasks, (ii) Learning with feedback from the programmatic evaluator, and (iii) Learning via self-exploration within the execution environment.",2024-04-15T20:45:39.915Z
NAIRR240144,170031,New,4/22/24,10/22/24,1264601,1264601,"An Empirical Evaluation of Accuracy, Robustness and Bias of Compressed Language Models","Srikumar, Vivek",University of Utah,0000-0003-0419-6568,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"It is not sufficient to ensure the safety, security and trustworthiness of today's largest AI systems. Indeed, given their resource constraints, compressed versions of AI systems will be deployed more often and ensuring their safety and robustness is important. To be effective, model compression methods must maintain predictive accuracy, while ensuring out-of-domain robustness. Furthermore, the resulting models should not show disparate predictive performance across societal groups. As a first step towards the safety of compressed models, our objective is to conduct the first comprehensive, multi-dimensional evaluation of compression techniques, focusing on accuracy, reliability, robustness, and bias of the resulting models.",2024-04-15T20:50:33.416Z
NAIRR240154,170044,New,4/22/24,10/22/24,1264614,1264614,Georgetown University Advanced Computing Resource Request for AI Education,"Hickman, James",Georgetown University,nairr-user-8ocqx,Artificial Intelligence and Intelligent Systems,502096,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"This proposal aims to acquire computational resources for student-led projects to bolster AI education, with the goal of facilitating workforce development in the Artificial intelligence space. The Department of Data Science and Analytics (DSAN) at Georgetown University boasts a sizable and diverse population of over 200 Master's level students, with an annual cohort size of approximately 120 students. Our goal is to secure computational resources that will enable our student body to delve into a wide array of issues at the intersection of data science, deep learning, and AI. This includes tasks such as testing, evaluating, verifying, and validating AI systems and models to ensure safe, secure and trustworthy AI. Our program offers a comprehensive range of AI and deep learning courses, including DSAN-6600: Introduction to deep learning, DSAN-6500: Image Mining and Computer Vision Analytics, DSAN-6650: Deep Reinforcement Learning, DSAN-5400: Computational Linguistics, DSAN 5800: Advanced Natural Language Processing, and DSAN-5810: NLP with Large Language Models. While these courses provide our students with a robust skill set in deep learning, their projects are currently significantly restricted by our limited access to GPU and super-computer resources. Therefore, our objective in submitting this proposal is to enhance our department's computational resources, thereby expanding the complexity of the models and methodologies accessible to our students.",2024-05-02T18:18:55.132Z
NAIRR240161,170046,New,4/22/24,10/22/24,1264616,1264616,LakeGPT: Building a Foundation Model for Aquatic Sciences,"Karpatne, Anuj",Virginia Tech,0000-0003-1647-3534,Ecology,502127,NAIRR Pilot Call for Early Allocations,NAIRR Pilot,"The goal of this project is to build a foundation model for aquatic sciences, termed Lake-GPT, to model a variety of processes related to the quality of water in lakes and reservoirs, using novel advances in the emerging field of ecology knowledge-guided machine learning (eco-KGML). Supported by an NSF grant from the Macrosystems Biology and NEON-Enabled Science (MSB-NES) program, our work is opening a new chapter of research in AI to model scientific systems where we have growing volumes of data collected from sensors as well as generated from model simulations. Our research in eco-KGML also makes a distinct departure from mainstream practices of building black-box AI models, that solely rely on supervision contained in data, to also leverage the wealth of scientific knowledge available in many domains including aquatic sciences (e.g., conservation laws of mass and energy) in diverse formats (as equations, rules, heuristics, or as mechanistic model runs) and with varying fidelities of knowledge (ranging from perfect and complete to imperfect and partial). Inspired by the success of large-scale foundation models in mainstream applications of computer vision and language understanding in the commercial arena, the domain of aquatic sciences is ripe with opportunities to investigate the effectiveness of foundation models for modeling scientific processes using both scientific knowledge and data.",2024-04-15T19:58:34.469Z
