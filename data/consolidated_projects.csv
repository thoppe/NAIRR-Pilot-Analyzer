requestNumber,requestId,beginDate,endDate,requestTitle,pi,piInstitution,fos,allocationType,abstract
NAIRR240007,1256555,2024-04-22,2024-10-22,Investigating Security Issues in Instruction-Tuned Large Language Code Models,"Hei, Xiali",University of Louisiana at Lafayette,Other Computer and Information Sciences,NAIRR PIlot,"The incorporation of AI coding tools into software engineering workflows raises significant security concerns. Malicious user prompts or training data attacks may result in AI-generated code containing software backdoors, logic corruption, unauthorized code execution, and other malicious behavior or software exploits. Previous studies have primarily focused on evaluating the robustness of large pre-trained code models like CodeBERT, GraphCodeBERT, and CodeT5 against adversarial examples in tasks such as code clone detection, vulnerability identification, and authorship attribution. However, security issues in advanced and instruction-tuned code LLMs like WizardCoder and CodeLlama have yet to be investigated.

To address this gap, the project aims to systematically uncover novel vulnerabilities and attack vectors introduced by instruction-tuned code LLMs, providing fundamental insights into their implications for cybersecurity and AI-incorporated software engineering workflows. The proposed work represents the first comprehensive investigation into the risks of code injection attacks against instruction-tuned code LLMs. Furthermore, we will develop a technique for identifying adversarial threats against instruction-tuned code LLMs, involving the detection of adversarially manipulated inputs and generated code before execution."
NAIRR240008,1253863,2024-04-22,2024-10-22,"Developing Trustworthy, Robust AI models for Science using Privacy Preserving Federated Learning","Madduri, Ravi",Argonne National Laboratory,Computer Science,NAIRR PIlot,"Federated learning (FL) is a collaborative learning approach where multiple data owners, referred to as clients, train a model together under the orchestration of a central server by sharing the model trained on their local datasets instead of sharing the data directly. FL enables creation of more robust models without the exposure of local datasets. However, FL by itself, does not guarantee the privacy of data, because the information extracted from the communication of FL algorithms can be accumulated and utilized to infer the private local data used for training. We developed Argonne Privacy Preserving Federated Learning framework (APPFL), with advances in differential privacy, to enable Privacy-Preserving Federated Learning (PPFL). We enabled training of AI models in a distributed setting across multiple institutions, where sensitive data are located, with the ability to scale on supercomputing resources to help create robust, trust-worthy AI models in biomedicine and smart grid applications where data privacy is essential. Setting up a secure federated learning experiment that needs high performance computational resources across distributed sites requires technical capabilities that may not be available for all. To lower the barrier to entry for leveraging PPFL and to enable domain experts in large institutions to utilize FL, we created the Argonne Privacy-Preserving Federated Learning as a service (APPFLx), which enables cross-silo PPFL using easy to use web interface for managing, deploying, analyzing, and visualizing PPFL experiments. 

APPFLx enables secure federations using end-to-end strong Identity and Access Management, where members can create a new federation or join an existing federation using their institutional identities, perform privacy-preserving training on datasets at their respective institutions and securely share the model weights with the service to enable secure aggregation. Existing PPFL frameworks typically involve downloading and configuring complex software, manually creating trust boundaries and identities to enable gradient aggregation, understanding of technical details of underlying deep learning software stack to enable distributed training which is cumbersome. 

APPFLx features include secure distributed training on heterogenous computing resources, choice of over half a dozen federation strategies, including synchronous and asynchronous strategies, integration with TensorBoard capabilities, interfaces to examine data distributions and resource consumption across different sites, detailed reports of different experiments, ability to use model architectures from GitHub or pre-trained models from HuggingFace model repository and the ability to set different hyperparameters of the experiments (like privacy budget to be used in training). 

In this proposal, we aim to create new AI models, fine-tune existing LLMs for biomedical tasks, evaluate various communication, privacy and performance issues inherent in FL."
NAIRR240009,1257064,2024-04-22,2024-10-22,Enhancing Generalization for Detecting AI-Synthesized Fake Multimedia,"Hu, Shu",Purdue University in Indianapolis,Computer Science,NAIRR PIlot,"DeepFake, a term increasingly mentioned in the news and social media, refers to highly realistic fake images, audios, and videos created using AI algorithms.  By creating illusions of an individual’s activities that did not occur in reality, DeepFakes can cause serious harm when they are weaponized. The recent detectors often show good results in intra-domain testing,  where training and testing data are generated using identical forgery techniques. However, real-world scenarios often involve testing data created via unknown forgery methods, differing from the training data. This discrepancy leads to subpar cross-domain detection performance, exemplifying the generalization problem in DeepFake detection. This issue poses a major hurdle for the practical application of existing detection methods. Without such generalization, the current DeepFake detection methods are failing to recognize DeepFakes generated by DeepFake makers using new or evolving techniques and therefore are susceptible to obsolescence easily. Rather than waiting for new DeepFakes that evade current forgery-specified detectors, we propose a proactive strategy with a novel framework. The framework aims to find common features in various forgeries, promoting learning in a model on a flattened loss landscape to improve the detector's ability to generalize. This project requests the NAIRR Pilot resources to enhance generalization in detecting novel DeepFakes, aligning with NAIRR's focus area on advancing ''Safe, Secure and Trustworthy AI'' and the core research aim of ``Improving accuracy, validity, and reliability of model performance, while controlling bias.'' The project's findings will be disseminated through publications in prestigious conferences and journals. All algorithms developed throughout this project will be made openly accessible as open-source software on GitHub."
NAIRR240010,1258651,2024-04-22,2024-10-22,"Training Foundation Models from Private, Federated Client Data","Fanti, Giulia",Carnegie Mellon University,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"Federated learning (FL) is arguably the most widely-adopted paradigm today for training machine learning models from private, on-device client data. However, with the rise of large language models, many models are too large to fit and train on-device in FL pipelines. Various solutions have been explored, including training sparse subsets of parameters on-device. In this proposal, we study a different paradigm for training foundation models from distributed, private client data: private synthetic data. We aim to study whether distributed clients can help a central server generate private synthetic data that can be used to centrally fine-tune foundation models. We hypothesize that this approach can be used to train private language models much more cheaply than  with FL."
NAIRR240011,1257794,2024-04-22,2024-10-22,Generative Artificial Intelligence and Racial Bias in Disease Representation,"Mankowski, Michal",NYU Langone,Clinical Medicine,NAIRR PIlot,"This research proposal aims to expand upon the ongoing study on generative Artificial Intelligence (genAI) and racial/sex bias, which investigates the depiction of patients with various diseases by genAI. We will study a broad range of diseases, particularly those with historical stigmatization, and incorporate a wide range of various AI models. The study will examine the sensitivity of genAI to linguistic framing of prompts in the context of different diseases such as cancer, diabetes, syphilis, etc. The research can potentially reveal important insights into understanding and mitigating racial biases and historical stigmatization in AI disease representation."
NAIRR240012,1259341,2024-04-22,2024-10-22,Aligning with Human Preferences for Safe Autonomous Systems,"Sadigh, Dorsa",Stanford University,Computer Science,NAIRR PIlot,"Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. In this work, we study the problem of learning robot policies and reward functions that are aligned with societal scale objectives and human preferences. This ensures the robot policies learned satisfy desirable safety specifications and can robustly act and interact in human spaces. We plan to leverage and refine large vision-language models that can act as proxy preferences of humans via human feedback. Specifically, we will pretrain and adapt robot policies initialized with these VLMs to achieve the alignment objective. Finally, we will evaluate these policies on real robotic hardware for assistive and home robotics tasks with 7-DoF manipulators."
NAIRR240016,1262014,2024-04-22,2024-10-22,Exploring Backdoor Attacks in Off-the-Shelf Unsupervised Domain Adaptation for Securing Medical Image Analysis,"Liu, Xiaofeng",Massachusetts General Hospital,"Informatics, Analytics and Information Science",NAIRR PIlot,"Recently, the off-the-shelf model for unsupervised domain adaptation (OSUDA) has been introduced to protect patient data privacy and intellectual property of the source domain without access to the labeled source domain data. Yet, our recent works found that an off-the-shelf (OS) diagnosis model, deliberately compromised by backdoor/Trojan attacks during the source domain training phase, can function as a parasite-host, disseminating the backdoor to the target domain model during the OSUDA stage. Therefore, backdoor attacks can pose a serious threat to the reliability of the target domain medical analysis systems. To our knowledge, this is the pioneer attempt at addressing backdoor attacks in medical image analysis for OSUDA. In this project, we propose to quantify the channel-wise backdoor sensitivity, and eliminate the backdoor infection by overwriting the backdoor-related channels to cut-off the infection. If it is successful, the project will have great potential to achieve a secured, privacy-protected, and cross-center generalizable AI system for medical image analysis."
NAIRR240020,1262279,2024-04-22,2024-10-22,Energy-based Robust and Safe Reinforcement Learning Via Diffusion Models,"Chen, Haipeng",William & Mary,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"This proposal aims to improve the existing MaxEnt Reinforcement Learning algorithm S2AC (Messaoud et al., 2024) by replacing the SVGD policy with a Diffusion Model-based policy. Since Diffusion Models are a class of generative models that are practically more stable than SVGD, we conjecture that the performance, especially robustness and safety of S2AC would be significantly enhanced by replacing the SVGD policy with Diffusion Model."
NAIRR240021,1262408,2024-04-22,2024-10-22,Determining an optimal initial fluid rescucitation regime for patients with sepsis using machine learning,"Shteyler, Vadim","University of California, San Francisco",Clinical Medicine,NAIRR PIlot,"I am a postdoctoral trainee on a TL1 grant doing research and learning machine learning tools for causal inference. This research aims to reduce mortality in patients with sepsis, a heterogenous life-threatening response to infection characterized by organ dysfunction and immune dysregulation; it impacts millions of people annually worldwide and has a 15-30% mortality. Early intravenous fluid administration and vasopressors remain the hallmarks of sepsis-associated hypotension and shock treatment. However, high-quality evidence for any specific initial fluid resuscitation strategy is lacking. Nonetheless, Surviving Sepsis Campaign (SSC) guidelines and the Centers for Medicare and Medicaid Services (CMS) Severe Sepsis/Septic Shock Early Management Bundle (SEP-1) recommend an initial 30 cc/Kg ideal body weight (IBW) fluid administration within 3 hours of diagnosis and, if hypotension persists, vasopressor initiation within 6 hours of diagnosis. To determine whether this approach is, indeed, beneficial for patients with sepsis, we propose to conduct this retrospective observational cohort study to estimate the average fluid volume and administration rate that is associated with the lowest hospital mortality for most patients, using Targeted Maximum Likelihood Estimation (TMLE) and Longitudinal TMLE (LTMLE) with the ensemble machine learner SuperLearner (SL)."
NAIRR240023,1263278,2024-04-22,2024-10-22,Interpretable Bayesian Deep Learning: From Neural Networks to Large Language Models,"Wang, Hao",Rutgers University - New Brunswick,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"Interpretability is one of the fundamental obstacles hindering the adoption and deployment of deep learning (DL) systems, from typical deep neural nets like CNNs/RNNs to large language models (LLMs) and large multimodal models (LMMs) like GPT-4(V), across various fields such as healthcare, e-commerce, and manufacturing. An ideal interpretable model should be able to produce symbolic representations understandable by humans (e.g., what diseases and symptoms each variable in the model represents), conform to conditional dependencies in the real world (e.g., whether the customer's purchase is affected by the product's price drop), and handle uncertainty in data (e.g., how certain the model is about the rainfall tomorrow). Unfortunately, DL as a connectionist approach does not natively support any of these desiderata.

This project aims to develop two sets of methodologies grounded in Bayesian deep learning: (1) ""Bayesian Deep Interpreters,"" which will interpret deep learning models through the use of graphical models that describe the conditional dependencies leading to current predictions. (2) ""Bayesian Deep Controllers,"" which will manage the predictions of deep learning models by adjusting specific random variables within the graphical models associated with the controlled models."
NAIRR240024,1263371,2024-04-22,2024-10-22,Evaluating and Mitigating Bias in Autonomous Patient Monitoring from ICU Videos,"Schulman, Kevin","Clinical Excellence Research Center (CERC), Stanford School of Medicine",Health Sciences,NAIRR PIlot,"As part of our research objective to advance ambient intelligence in healthcare, we propose developing computer vision models to predict Richmond Agitation-Sedation Scale (RASS) scores from video streams of cameras in ICU patient rooms. RASS score is a critical indicator of a patient’s level of consciousness and agitation. By accurately predicting RASS scores in real time, we can help clinicians manage patient sedation, improve patient outcomes, and enhance the quality of care provided. Importantly, our model development process incorporates robust measures to mitigate bias related to race and demographics, ensuring equitable predictions across diverse patient populations."
NAIRR240026,1263433,2024-05-22,2024-11-22,AI-Driven Unbiased Career-Life Assessment for the ADVANCEment Marginalized STEM Faculty,"Gupta, Anju",The University of Toledo,"Informatics, Analytics and Information Science",NAIRR PIlot,"The long-term goal of this project is to transform campus culture and policies to create an inclusive campus for women and marginalized faculty. The ADVANCE-University of Toledo (UToledo), Rochester Institute of Technology (RIT) and George Mason University (GMU) sites focus on I) identification and remediation of gender-based biases and barriers preventing junior to mid-career women faculty from growing professionally, and II) creation of programs for marginalized faculty in life event transitions, and professional development grants for vulnerable career junctures. Through NAIRR resources, the team will design a tool capable of automated analysis of faculty career life surveys conducted by ADVANCE sites to understand the barriers and inequities among faculty with respect to a) compensation satisfaction, b) teaching load, c) academic engagement, d) professional and institutional environment, e) career development and planning, and f) awards and recognition.  This specialized tool will include both numerical statistical analysis as well as natural language processing (NLP)-based language tools including customized large language models (LLMs) for both quantitative and qualitative survey analysis.  It is expected that the results from the tool will inform best practices in diverse recruiting, retention and promotion, removal of professional development barriers, and creation of institutional framework that promotes equitable treatment among diverse faculty."
NAIRR240027,1263374,2024-04-22,2024-10-22,Enhancing Scalability and Privacy of Federated LLMs,"Ding, Yufei","University of California, San Diego",Computer Science,NAIRR PIlot,"This proposal explores the evolving landscape of Large Language Models (LLMs) like GPT, LLaMa, and PaLM, which have revolutionized areas including ChatBot interaction, time series forecasting, and code generation. Central to LLMs' effectiveness is the scale of their architecture and the extent of training data. However, the reliance on extensive, diverse datasets challenges the capacity of public data, necessitating the integration of private domain data. This integration, while promising, is hindered by privacy and competitive concerns. To navigate these challenges, the proposal emphasizes Federated Learning (FL) for LLMs. FL enables decentralized training across multiple private domains without relocating the data, addressing privacy issues. Yet, FL introduces systemic challenges, such as hardware heterogeneity and connectivity variances, which can compromise training effectiveness. Additionally, privacy concerns in FL are amplified in LLMs due to the influential role of fewer participants, raising the risk of peer-to-peer data breaches. The research focuses on two key areas: developing a Scalable Federated Learning System for LLMs and formulating a Security and Privacy Preserving Algorithm for their training. This approach aims to refine federated training processes and algorithms, setting a benchmark in federated LLM training. The anticipated outcome is a versatile, secure federated learning framework applicable across various domains, demonstrating the extensive potential of FL in advancing LLMs."
NAIRR240028,1263700,2024-04-22,2024-10-22,Build Reliable and Secure AI Surrogates for Large-Scale Scientific Applications with Portability Performance Analysis,"Dong, Wenqian",Florida International University,Applied Computer Science,NAIRR PIlot,"Large-scale scientific simulations drive scientific and engineering discovery across many domains, but face performance problems. These simulations typically involve complex physics computations, such as (non-)linear programming problems in transportation, manufacturing, robotics, power grids, and renewable energy systems, which causes unaffordable running time. However, the computation components in the simulations are difficult to scale efficiently on high-performance hardware. 

In the next decade, Artificial Intelligence (AI) and Machine Learning (ML) may revolutionize the natural sciences, enhancing our capacity to model and predict natural occurrences. This could herald a new era of scientific exploration, bringing significant advancements across sectors from drug development to renewable energy.  By employing reverse-engineering and automatic learning methodologies it is often possible to solve complex, unstructured problems with a fraction of the computing power and execution time needed by traditional direct and first-principle methods. A deep neural network provides researchers with a powerful tool to learn the structure of physical phenomena directly from Nature, rather than having to explain the causal relationships through the direct application of physics law. Researchers have found that using AI techniques to act as surrogates for the computation-intensive parts of High-performance Computing (HPC) programs can achieve excellent accelerations."
NAIRR240030,1263963,2024-04-22,2024-10-22,Improving Factuality in Automated Course Logistics Question Answering,"Lan, Andrew",University of Massachusetts Amherst,Educational Sciences,NAIRR PIlot,"Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We have collected a dataset, which we call SyllabusQA, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Our goal is to benchmark several strong baselines on this task, from large language model (LLM) prompting to retrieval-augmented generation (RAG), to investigate whether existing methods can perform well in terms of both textual similarity and fact precision."
NAIRR240031,1263980,2024-04-22,2024-10-22,"Advanced Training for Protein Diffusion, Binder Prediction, and Antibody Design","Baker, David",University of Washington Howard Hughes Medical Institute,Biochemistry and Molecular Biology,NAIRR PIlot,"Advances in deep learning have enabled accurate protein structure prediction and are massively speeding up biochemical discoveries. We recently developed RoseTTAFold All-Atom and RFDiffusion, allowing prediction and generation of complicated biomolecules, including non-amino acids. While seeing early success, current models encounter challenges for designing binders to polar flexible ligands, antibody-protein interactions, and enzyme catalytic sites. We propose to fine-tuning our current models for specific problems, such as antibodies and polar ligands, to quickly improve the model performance even with limited training resources and enable us to tackle problems currently inaccessible but more clinically related."
NAIRR240032,1262184,2024-04-22,2024-10-22,Developing a benchmark hydrologic dataset and a fast AI surrogate model for assessing climate impacts on mountainous hillslopes,"Wang, Lijing",University of Connecticut,Hydrology and Water Resources,NAIRR PIlot,"Mountainous watersheds serve as the water tower for downstream rivers. However, observations of discharge and groundwater levels are limited at the subcatchment level, and using hydrologic modeling directly at the subcatchment scale without observations will not provide realistic predictions. To address this sparse data gap, this project aims to simulate stochastic hydrologic responses across 2,000 mountainous hillslopes under various climate scenarios and uncertain subsurface and surface properties. The baseline mountainous hillslope model we used has integrated intensive geophysical and hydrologic datasets. The simulated 2,000 hydrologic responses become an AI-ready dataset to benchmark realistic hydrologic variations. Additionally, leveraging the Neural Operator, we plan to develop a fast AI surrogate model to quickly predict snowpack, discharge, and groundwater levels, substituting time-intensive hydrologic simulations. This pilot study not only advances hillslope hydrologic models by incorporating realistic subsurface and surface uncertainties but also contributes an openly accessible hydrologic dataset and a pretrained AI model for rapid hydrologic forecasting in mountainous regions, supporting water budgeting and climate impact assessments."
NAIRR240038,1264268,2024-04-22,2024-10-22,Towards Mechanistic Audits of LLMs,"Crovella, Mark",Boston University,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"The safety of machine learning tools is a critical societal concern.  The imment wide deployment of LLMs in society elevates the need for principled methods for auditing LLMs for socially undesirable behavior.   While previous attempts to perform algorithmic audits are often black-box, we propose to explore the white-box auditing of open source LLMs via the techniques of mechanistic interpretability.   This project will develop a large test suite of datasets corresponding to socially relevant concepts.  We will then use these datasets to probe a suite of LLMs to determine for which concepts and models the linear representation hypothesis holds.  In those cases, we will examine the resulting linear representations for evidence of undesirable interaction, for example, through interference due to superposition.  All datasets and results will be published in the open literature."
NAIRR240040,1264279,2024-04-22,2024-10-22,The Development Safety of Large Foundation Models,"Yang, Tianbao",Texas A&M University,Computer Science,NAIRR PIlot,"This project will investigate an importance issue of continual developing large foundation models. While large foundation models have demonstrated remarkable capabilities across various tasks, they are not without limitations, often necessitating iterative updates to enhance their performance in specific domains or tasks. In order to enhance the system’s ability to adapt to the changing and complex world, it is necessary to undergo multiple iterations of model development, which involve collecting new data and training new
models. However, this iterative model development process raises significant safety issues that have been overlooked by existing studies in AI, i.e., the model development for acquiring new safety features may inadvertently lose the previously ensured safety features of the old model. This project will investigate novel approaches to address these issues."
NAIRR240042,1264297,2024-04-22,2024-10-22,Reinforcement Learning from Conversational Signals,"Artzi, Yoav",Cornell University,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"We propose to fine-tune language models from interaction with users without explicit feedback, but instead from signals arising from how humans respond to system outputs in multi-turn conversational interactions. Unlike explicit human feedback, these signals require no annotation effort beyond the interaction itself. We develop an approach to learn from human response to system utterances to improve an underlying large language  model. We use two interaction scenarios for our studies and experiments, and evaluate through deployment with human users."
NAIRR240044,1264312,2024-04-22,2024-10-22,Training and Benchmarking Three Different Foundation Models on Public Histopathology Gigapixel Images for Whole Slide Image Representation,"Tizhoosh, Hamid",Mayo Clinic ,Performance Evaluation and Benchmarking,NAIRR PIlot,"This proposal addresses the challenge of transforming whole slide images (WSIs) into actionable vectors for different computational pathology (CP) tasks. Despite recent progress in leveraging artificial intelligence (AI) in CP, WSIs pose unique challenges due to their large size and complex features. Our objective is to secure resources to train and benchmark three foundation models tailored for WSI representation. We aim to utilize publicly available datasets to conduct the largest-ever effort in training and benchmarking models at the WSI level. The selected models include LongNet, LambdaNetworks, and RWKV, each offering distinct approaches to long sequence analysis. Leveraging diverse datasets such as TCGA, PANDA, CAMELYON16, BRACS, and BCNB, we will ensure comprehensive coverage for robust training and evaluation. The evaluation will assess the performance of each model across relevant metrics, providing standardized findings for comparison. Given the massive size of WSIs, substantial computational and storage resources are required. Successful implementation of this proposal will advance research in CP, driving innovation and clinical translation. By providing comprehensive evaluations, this initiative empowers the research community to make informed decisions, ultimately improving patient outcomes."
NAIRR240045,1264320,2024-04-22,2024-10-22,"Guardians of Integrity in AI: Establishing Trust, Originality, and Ethical Standards (GATES)","Huang, Furong",University of Maryland,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"The advent of generative AI has ushered in unprecedented capabilities alongside significant ethical and security challenges. “Guardians of Integrity in AI: Establishing Trust, Originality, and Ethical Standards (GATES)” proposes an innovative research agenda aimed at fortifying the ethical backbone of artificial intelligence. This project targets three pivotal areas: the detection of AI-generated content to combat misinformation, the protection of intellectual property rights to uphold the originality and fairness in AI-generated works, and the development of trustworthy foundation models for enhanced human-AI collaboration. Through GATES, we intend to develop robust methodologies and frameworks that ensure AI systems are safe, secure, and aligned with human values. This initiative not only addresses immediate concerns within the AI community but also lays the groundwork for sustainable, ethical AI development, resonating with the goals of the NAIRR Pilot initiative. By integrating advanced computational techniques with ethical principles, GATES aims to lead the way in establishing a new standard for trustworthy AI, ensuring that these powerful technologies augment human capabilities without compromising ethical standards."
NAIRR240046,1264321,2024-04-22,2024-10-22,Enhancing LM Adaptation toward Safe and Trustworthy AI,"Hajishirzi, Hanna",University of Washington,Computer Science,NAIRR PIlot,"The capabilities of large language models (LMs) to follow user requests have been progressing rapidly through a wide range of openly available models, datasets, and training methods. In the meantime, we need to make these models safe, secure, and trustworthy. There has been rapid progress in both improving capabilities and safety of language models. Our team at UW are at the forefront of building open-source language models and adapting them to new capabilities and safety measures~\citep{wang2023far}, and making them trustworthy~\cite{asai-etal-2023-retrieval}.
Since the release of our state-of-the-art models~\citep{wang2023far},  there have been a number of significant advances in almost all aspects of  language model adaptation, from the release of improved finetuning datasets~\citep{UltraChat, cui2023ultrafeedback}, to  increasingly powerful base models~\citep{touvron2023llama,jiang2023mistral}, to powerful and accessible adaptation methods for combining these components~\citep{rafailov2023direct, dettmers2023qlora}, and retrieval-augmentation models~\cite{asai-etal-2023-retrieval}. Building on our team's prior expertise, we propose to introduce and investigate approaches to make language models safe  and trustworthy."
NAIRR240047,1264313,2024-05-22,2024-11-22,Toward Reliable Large Language Models via Conformal Prediction,"Cheng, Lu",,Computer Science,NAIRR PIlot,"This project aims to address the pervasive challenge of quantifying uncertainty in large language models (LLMs) without logit-access. Conformal Prediction (CP), known for its model-agnostic and distribution-free features, is a desired approach for various LLMs and data distributions. However, existing CP methods for LLMs typically assume access to the logits, which are unavailable for some API-only LLMs. In addition, logits are known to be miscalibrated, potentially leading to degraded CP performance. To tackle these challenges, we will introduce a novel CP method that leverages diverse sources of uncertainty."
NAIRR240050,1264322,2024-05-22,2024-11-22,Safety Tests as Standards for Large Language Models,"McGregor, Sean",UL Research Institutes,Performance Evaluation and Benchmarking,NAIRR PIlot,"The Digital Safety Research Institutes (DSRI), of UL Research Institutes aims to contribute to AI safety research via the evaluation of programmatic safety cases. A “safety case” is a structured argument intended to justify that a system is acceptably safe for a specific application in a specific operating environment. General-purpose systems operating in open-ended domains (i.e., large language models) violate both the application and environment specificity requirements of safety engineering. Conceding that general-purpose systems will increasingly be deployed; our approach is to greatly accelerate the generation of safety cases to meet the scale of modern systems. We envision an ecosystem where a growing collection of sequestered programmatic assessments are curated by subject matter experts to assess the safety of competing AI models. Through the creation and programmatic application of thousands of assessments, our research approach addresses the general-purpose setting with scale and speed. In this resource request, we seek to demonstrate long term viability of laboriously prepared safety cases via the scale provided by the National Artificial Intelligence Research Resource."
NAIRR240051,1264333,2024-04-22,2024-10-22,Neuro-inspired Oversight for Safe and Trustworthy Large Language Models,"Kim, Edward",Drexel University,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"We have observed that improving instruction following is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines.  We have previously showed that larger parameter models exhibit superior capability in following instructions that require overriding both internal knowledge and contextual cues, demonstrating a high degree of obedience.  However, our research also highlights a fundamental issue between enhancing a model's ability to override instructions and maintaining adherence to safety protocols and guidelines.  Specifically, the most responsive instruction following LLMs are the ones that can be most easily jailbroken.

In our work, we believe that the architecture of the human brain illustrates a critical point, there is a separation between language understanding and moderation.  This is supported by functional mapping studies that show the language understanding center, e.g. Wernicke’s area, and speech moderation and production area, Broca’s area and surrounding Frontal Lobe, are in two distant and separate areas of the brain.  We are pursuing research in LLM moderation through the lens of biological intelligence."
NAIRR240052,1264332,2024-04-22,2024-10-22,"Pre-training a generative selective state space model, the Mamba model, on UCSF-specific deidentified clinical notes and time-series structured data","Sushil, Madhumita","University of California, San Francisco",Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"While large language models (LLMs) have made unprecedented advances in the general domain, their advances in medicine are limited by access to large datasets and advanced computing infrastructure. Existing publicly available LLMs are not always sufficient for healthcare-specific use, since they either lack domain knowledge, are expensive to use, or cannot be accessed in a HIPAA-compliant setting. To enable more widespread use of AI within medicine, we aim to develop an effective and task-independent foundation model for medicine trained on a large and diverse corpus of deidentified patient data from electronic health records at University of California San Francisco. A recent family of models, selective state space models, have demonstrated promising capabilities in long sequence processing, providing competitive performance to transformers-based models. In this study, we aim to develop a longitudinal selective state space LLM on medical data by encoding sequential knowledge from clinical encounters for generating future clinical states and actions."
NAIRR240054,1264335,2024-05-22,2024-11-22,Single-cell foundation model for plant biology,"Xu, Dong",University Of Missouri-Columbia,Biochemistry and Molecular Biology,NAIRR PIlot,"This project aims to pioneer the development of the first plant biology-specific single-cell foundation model, addressing the unique challenges and complexities of plant single-cell analysis. By leveraging advanced AI and machine learning techniques, the model will be designed to navigate the intricacies of plant biology, including gene lexicons, expression patterns, and cellular typologies. The project will involve data collection and preprocessing, foundation model construction, model validation and adaptation, and eventual model deployment and publication. The model is expected to enhance plant biology research with comprehensive validation, offering insights into plant development, physiology, and response to environmental stresses."
NAIRR240055,1264338,2024-04-22,2024-10-22,Securing Healthcare Privacy: Rendering Large-Scale Unlearnable Medical Imaging Data to Prevent Data Leaks,"Huo, Yuankai",Vanderbilt University,Computer Science,NAIRR PIlot,"This proposal outlines an innovative secure and trustworthy AI approach to safeguard healthcare data from unauthorized use in AI model training through the development of unlearnable examples (UE). As AI models, including foundation models like ChatGPT, SAM, and Sora, increasingly raise privacy and safety concerns, especially with the unauthorized use of patient data, the National Artificial Intelligence Research Resource (NAIRR) project aims to pioneer privacy-ensured radiological AI. By embedding invisible noise in over 2 million radiological images from MRI and CT scans, this project will generate label-agnostic UE models that protect medical imaging data against misuse in future AI training, without compromising their clinical utility.

The project will leverage cutting-edge UE algorithms, high-performance computing, and Vision Transformer models to achieve its objectives. It addresses the urgent need for robust privacy measures in healthcare data, responding to the risks posed by the repurposing of medical imaging for AI training without explicit consent. The proposal also outlines a comprehensive strategy to overcome the challenges of developing UE models for healthcare, including the need for large-scale medical image datasets and substantial computational resources.

Aim 1 focuses on rendering large-scale medical imaging data unlearnable, while Aim 2 assesses the effectiveness of UE models against various target models and advanced defenses. The project plans a six-month timeline to achieve its goals, aiming for outcomes that will be shared in leading academic venues.

The success of this NAIRR project is expected to significantly advance the field of AI healthcare data security by utilizing Vanderbilt’s clinical imaging data in conjunction with DOE’s Leadership Computing Facility. This will lay the groundwork for future collaborations and open new avenues for healthcare and high-performance computing (HPC) AI research, ensuring privacy and intellectual property protections for sensitive healthcare data."
NAIRR240057,1264357,2024-04-22,2024-10-22,Unified Representation Learning,"Shrivastava, Abhinav","University of Maryland, College Park",Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"Deep learning has dominated the fields of generative modeling and discriminative visual recognition for the past decade. For generative tasks, a deep learning model seeks to synthesize or edit parts of images, while for discriminative tasks, it learns to label images or parts of images.  We argue that both of these can be considered complementary and intuitively should help each other because both need a semantic understanding of the underlying structure. We will develop unsupervised unified representations, for generative and discriminative tasks, to be used as general-purpose representations for various downstream tasks like image recognition, reconstruction, and synthesis. Such unified models can be efficiently finetuned for multiple downstream tasks, as opposed to having to pre-train large, expensive models separately for different tasks."
NAIRR240063,1264501,2024-06-06,2024-12-06,A Metric-learning Paradigm for Verifying Origination of Textual Contents,"Le, Linh",Kennesaw State University,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"Breakthroughs in Large Language Models (LLMs) has enabled Artificial Intelligence (AI) assistant systems that can provide quality information with conveniences. An issue is paralleling the advantages, however. LLM generated contents are problematic in that they are seemingly indistinguishable from that of human, which cause many issues in areas like science, education, information security, etc. Furthermore, approaches in AI synthesized content detection are either computationally expensive or need accesses to the LLMs' internal computations, both of which hinder their public access. Specifically, the detection is performed through evaluating the similarity metric between the given text to an equivalent example generated by LLMs and through that determining the former's origination. Being able to utilizing the LLMs themselves externally, computational resources will be less demanding in the detection model side. Overall, research components of this project include 1) developing metric models that measure similarity among AI and human texts, 2) developing models that reconstruct contexts for input texts, 3) adapting the framework to use cases including long documents, emails, and conversations, and 4) adapting models to unknown LLMs."
NAIRR240072,1264513,2024-06-06,2024-12-06,Development of Foundational Large and Multimodal Language Models for Hydrology Research and Education,"Demir, Ibrahim",University of Iowa,Hydrology and Water Resources,NAIRR PIlot,"This project aims to revolutionize the field of hydrology by developing a Hydrological Foundational Large Language Model (LLM) and a Multimodal Large Language Model (MLLM). These advanced artificial intelligence models will be capable of processing and generating hydrological knowledge from both textual and non-textual data sources. The initiative seeks to enhance data accessibility, analysis, and the generation of insights for stakeholders in hydrology, including researchers, educators, practitioners, and policymakers. By leveraging vast datasets, including academic literature, sensor data, satellite imagery and maps, the project will train new models to support a wide range of applications. These applications include facilitating research hypothesis generation, creating dynamic educational materials, and informing water management and policy decisions. The development process involves data collection and preprocessing, model training, integration testing, and the creation of user-friendly interfaces, culminating in the public release of the models and findings. This endeavor is expected to significantly advance the understanding and management of water resources, promoting innovation and collaboration across the hydrology domain."
NAIRR240074,1264284,2024-06-06,2024-12-06,Scaling layer-wise feature shaping for robust deep networks: next-generation architectures inspired by communication theory,"Madhow, Upamanyu","University of California, Santa Barbara",Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"The exponential growth of AI is driven by rapid and continuing innovations in deep neural networks (DNNs). The ability to use massive amounts of computation and training data has led to stunning advances in a diversity of areas, including well-established applications such as recommendation systems and computer vision as well as emerging applications such as generative AI and scientific discovery.  However, AI researchers remain well aware that, despite their runaway success, DNNs lack robustness against noise, natural data distortions and adversarial attacks (the latter are small perturbations to the input designed to disrupt DNN performance).  The common approach to increase robustness in DNNs is data augmentation: the data used for training the DNN is augmented by the kinds of distortions that we wish to be resilient against.  However, it is difficult to scale this approach to enhance general-purpose robustness against a wide variety of distortions, including ones that we do not anticipate.

We have recently developed an approach for next-generation DNN architectures that draws upon ideas from communication theory, which forms the basis for today’s wireless cellular and WiFi systems.  A DNN processes the data in multiple layers, and the key idea is to modify the training objective so as to enhance the “signal-to-noise ratio,” a common measure of resilience in communication systems, at each layer.  This is done by adding layer-wise training objectives to shape the features extracted by each layer, in addition to the conventional end-to-end training objective.  We also modify the inference path of the DNN in accordance with communication-theoretic principles, introducing nonlinearities that attenuate distortions and perturbations.  Preliminary results on standard image datasets, based on modifying a single DNN layer using our approach, demonstrate significant gains in robustness with and without data augmentation.

The goal of this NAIRR project is to use the available computational resources to scale our experiments to larger datasets, replacing more layers using our approach. The goal is to provide comprehensive design guidelines and larger-scale proofs of concept of our approach, in order to establish this approach as an essential component of the DNN design toolkit.  We plan to engage the AI R&D community by continuing to open source our code, and by publishing our work in high-profile conferences and workshops."
NAIRR240082,1264498,2024-05-22,2024-11-22,Advancing Machine Learning Systems' Resilience Against Adversarial Attacks,"Shen, Haiying",University of Virginia,Computer Science,NAIRR PIlot,"We request computing resources for a project aimed at enhancing the resilience of machine learning systems against adversarial attacks. The research objectives are twofold. Firstly, we aim to assess the impact of adversarial attacks on large-scale deep learning models across diverse datasets in a distributed environment utilizing model parallelism. Secondly, we intend to devise a robust system to mitigate adversarial attacks on deep learning models, thereby preserving their reliability and trustworthiness during distributed training. Our goal is to construct a reliable machine learning system capable of addressing adversarial attack challenges in distributed training under model parallelism. This entails evaluating the trustworthiness of nodes and replacing untrusted ones with trusted ones based on computed trust scores to ensure the continuity of training tasks. Additionally, we will ensure the trustworthiness of parameters for model parallelism, including intermediate outputs and model parameter updates."
NAIRR240083,1264500,2024-06-06,2024-12-06,Improving the Reliability of Large Multimodal Models for Radiology with Confidence Estimates,"Rajpurkar, Pranav",Harvard Medical School,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"Our project aims to advance artificial intelligence (AI) assisted radiology report generation for chest X-rays and abdominal computed tomography (CT) scans by developing a system that not only performs state-of-the-art clinically accurate medical report generation but also integrates calibrated confidence estimates for each individual statement in its output. This effort addresses the critical issue of automation bias, where the over-reliance on AI-generated reports could lead to diagnostic errors. By benchmarking and rigorously evaluating various confidence estimation methods, our project aims to minimize these risks. Incorporating confidence scores at the level of individual statements is key to enhancing AI-assisted reporting workflows, allowing radiologists to identify claims needing further scrutiny more efficiently. This strategy is expected to improve the accuracy and reliability of AI-supported radiology reporting."
NAIRR240084,1264529,2024-06-06,2024-12-06,Auditing data provenance for fine-tuned language models,"Venkatasubramanian, Suresh",Brown University,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"The data used to train a language model is one of  the most valuable pieces of information we can have when assessing the performance and behavior of the model. Pre-trained models, especially large base models like GPT, BERT, and T5, along with their more recent successors like LLaMA 2, Mistral, and GPT-4, are trained on vast amounts of data taken from all across the web, and there is little to no transparency about what data is used and how. 

On the other hand, fine-tuned models make use of smaller and more customized data sets in order to tune a model for a specific task, such as breast cancer diagnosis from mammography images, customer reviews analysis for sentiment, and enhancing utility in dialog applications. Many of these data sets are well-known: the Data Provenance Initiative lists over $1800$ such data sets used in fine tuning, and provides detailed information about the data sets and their origin. It is therefore far more plausible that we can determine what data sets were used to fine-tune a model. This is also much more valuable information because the data set used for fine-tuning has a much more immediate effect on model behavior and thus helps us understand more clearly why models might produce the prompt responses that they do. 

The goal of this research project is to develop tools that given a dataset d and a fine-tuned model m, can determine whether d was used in fine-tuning to produce m either wholly or partially."
NAIRR240086,1263350,2024-06-06,2024-12-06,Exploring Heterogeneous Datacenter Design for Sustainable AI Deployment,"Brooks, David",Harvard University,Computer Science,NAIRR PIlot,"As we aim to advance the study of Safe, Secure, and Trustworthy AI, we must do so in a sustainable and efficient manner. The computing industry is forecasted to consume as much as 7-20% of global energy consumption by 2030 – a trend that is only accelerating given that ML models are becoming larger and more complex. Recent proposed legislation aimed at understanding the carbon footprint of AI encourages quantification of the designing and deploying AI at the datacenter-scale from an environmental perspective. While a plethora of novel hardware platforms have recently sought to accelerate AI workloads, there is a high cost to determining the optimal hardware and configuration for a given model, including a large sustainability footprint. To this end, we propose to construct a model to allow for the sustainable and efficient allocation of resources onto an array of novel AI accelerators. To do so, we will utilize the Argonne Leadership Computing Facility (ALCF) AI Testbed to construct a performance model that will allow researchers to predict the optimal hardware configuration for their workload. We will leverage the diversity of AI accelerators available in the ALCF testbed, including Groq and Cerebras, to measure how given workloads perform while constructing our model. We then aim to explore how to effectively schedule workloads across an array of heterogeneous accelerators, optimizing for both performance and carbon footprint."
NAIRR240088,1264511,2024-04-22,2024-10-22,Use of AI and sub-meter resolution satellite imagery to map permafrost thaw disturbances and human-built infrastructure at pan-Arctic scale,"Witharana, Chandi","University of Connecticut (Storrs, CT)",Geology and Solid Earth Sciences,NAIRR PIlot,"The Arctic is experiencing more rapid warming than many other regions leading to degradation of permafrost, - the permanently frozen earth materials that remain at 0°C or below for two or more consecutive years-, at an alarming rate. Widespread permafrost thaw induced disturbances negatively impact an array of natural processes, altering hydrology, vegetation dynamics and biogeochemical fluxes, and posing serious threat to human-built environment by damaging buildings and critical infrastructure sitting on permafrost-affected ground.  Permafrost thaw disturbances and microtopographic transitions are no longer localized phenomena but extend across the Arctic. Conventional field surveys and local-scale monitoring mechanisms are unable to offer synoptic assessments of permafrost degradations and ramifications on natural and human-built environments.  Consequently, there is a dire need for new monitoring tools to precisely map permafrost landforms and thaw disturbances and monitor their changes over time and assess the permafrost thaw risk on human-built infrastructure. 
Permafrost dominated landscape in the Arctic harbor a variety of distinctive landforms shaped by the freezing and thawing of the ground.  Ice-wedge polygons (IWPs), the most conspicuous and critical microtopographic formation found in the cold continuous permafrost regions, are underlain by several meter-wide and deep ice-wedges that form a network across the tundra. The microtopography associated with IWPs dictates the Arctic ecosystem from local to regional scales due to the impacts on the flow and storage of water and consequently alter vegetation and carbon dynamics. Field and remote sensing-based studies reported ice-wedge degradation transformation of low-centered polygons into high-centered polygons due to permafrost thaw at several locations across the Arctic tundra. Spatially patchy and temporally sparse knowledge on ice -wedge polygon system’s dynamics inevitably lead to uncertainties in regional and pan-Arctic estimates of carbon, water, and energy fluxes. Closing such critical information gaps and improving multi-scale models require objective and detailed geospatial data sets consolidating the ice wedge polygon extent and their prevailing successional stages.
The entire Arctic has been imaged by sub-meter resolution Maxar satellite sensors multiple times during the last two decades. Data repositories holding millions of Maxar images are now at petabyte scale. However, comprehensive Pan-Arctic science products derived from this imagery are scarce. This is largely due to data size (big data) and semantic complexity of satellite images. With the funding from NSF Polar Programs (award #s: 1927723, 1827872, 1927720), we pursue the first-of-its-kind permafrost science use case that capitalizes on a large volume of commercial satellite imagery to create the first Pan-Artic scale ice-wedge polygon map covering > 5 million km2 in the Arctic. We harnessed thousands of Maxar imagery, deep learning CNNs, and high-performance computing resources to develop a scalable and reproducible image-to-assessment pipeline. The operational implementation of our mapping pipeline on high-performance computing systems has detected more than one billion individual IWPs. The deep learning model predictions comprise spatial outlining of IWPs coupled with their classification and geometrical attributes at individual IWP-level. The resulting Pan-Arctic ice-wedge polygon map holds significant potential to serve a diverse user community, facilitating a deeper understanding of the intricate and interconnected processes governing the evolution of the ice-wedge polygon tundra landscape. 
The technical goal of this proposal is to improve the performance of our GeoAI pipeline by integrating vision transformers and extending its capabilities to map other important features. Self-attention-based neural net architectures, in particular Transformers have shown extraordinary success in multiple domains, especially in natural language processing (e.g., Chat GPT). Since foundation models are trained on internet-scale training data sets, they can be generalized with zero-shot and few-shot learning to a new task. The introduction of Segment Anything Model (SAM) is an attempt to come out with a foundation model for image segmentation tasks. SAM enables image segmentation without having to train the model on a large set of training samples (zero-shot learning). As with ChatGPT, SAM also uses a transformer-based approach. DL is rapidly moving away from the CNN-based computer vision models to vision transformers (ViTs). Currently, ViT-based models exhibit great promise on benchmark segmentation datasets. However, one major challenge is in adopting ViTs from everyday images to be able to segment geo-objects in high resolution satellite images. Continued access to HPC resources is critical for us to improve the existing DL pipelines via the introduction of new technologies, training and tuning of models, and to create new geospatial map products and reach our new aims successfully. The central objective of this proposal is to request HPC resources that are necessary to develop, train and validate transformer-based AI models."
NAIRR240089,1262392,2024-06-06,2024-12-06,Massive virtual screening of small molecules for high-affinity ASGPR ligands in MoDEs,"Lee, Ho-Joon",Yale School of Medicine,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"Accurate prediction of binding affinity of a drug ligand bound to a target molecule such as a protein is of central importance in early drug discovery. Development of advanced computational tools has enabled better prediction of ligand-protein binding affinity with applications to high-throughput drug screening or virtual screening. To maximize the power of existing tools in a synergistic and scalable way with less model-specific bias, we recently developed a meta-modeling framework of ligand-protein binding affinity prediction by ensembling structure-based classical docking tools and sequence-based deep learning models, achieving state-of-the-art performances compared to exclusively structure-based deep learning models. On the other hand, a novel therapeutic modality has been recently advancing by targeted protein degradation technologies based on hetero-bifunctional compounds. One of them is a molecular degrader of extracellular proteins or MoDEs, which recruits an extracellular protein of interest for lysosomal degradation by exploiting the asialoglycoprotein receptor or ASGPR.

The goal of this proposal is to improve our sequence-based meta-models using larger training datasets to advance more trustworthy AI and carry out virtual screening of 51,000 compounds from the Therma Fisher Maybridge Screening Library to identify high-affinity ASGPR ligands. In a preliminary study, our pre-trained meta-models was validated against a benchmark set of 96 experimentally validated ASGPR ligands with a Pearson correlation of up to 0.52. We aim to improve accuracy, validity, and reliability of model performance, while controlling model-specific bias and vulnerability through our meta-modeling framework. The potential impact of this proposal will be discovery of optimal MoDEs as effective drugs in broad therapeutic applications."
NAIRR240091,1263968,2024-04-22,2024-10-22,Privacy-Preserving Synthetic Data Generation for AI Research,"De Cock, Martine",University of Washington Tacoma,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"Data is the lifeblood of AI. Much of the most valuable data in the nation however is siloed in research centers, hospitals, banks, etc. The long and onerous processes that researchers have to go through to access each silo is causing a substantial underutilization of AI in many of the most important domains, including healthcare, genomics, justice, education, and finance. Synthetic data generation (SDG) offers an appealing solution to make data more broadly available for AI research while mitigating privacy concerns. Current SDG algorithms however lack provisions for protection of input privacy. Our goal is to leverage the NAIRR infrastructure to make it possible for data holders to contribute their data to an SDG process without disclosing that data in an unencrypted manner. The enhanced privacy brought by the ability to train synthetic data generators over encrypted data comes at a significant computational cost, which is a justifiable price to pay in very sensitive domains like healthcare and genomics."
NAIRR240092,1264326,2024-05-22,2024-11-22,Evaluation of LLMs as interactive agents for higher education,"Banavar, Mahesh",Clarkson University,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,We propose to use LLMs as interactive tools to benefit both educators and students. The first step towards building these tools is to evaluate the suitability of various LLMs for this task. We will consider (1) pre-trained LLMs with no additional training provided; (2) a trained GPT by customizing OpenAI’s GPT 4; and (3) custom-trained open source LLMs such as LLaMa 2 and DLite.
NAIRR240093,1264549,2024-05-22,2024-11-22,GPU-accelerated high-performance computing to supercharge foundational AI models for pandemic prediction,"Bhattacharya, Debswapna",Virginia Tech,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"The COVID-19 pandemic and its consequences, from medical to economic to societal, serve as a recent example of the devastation pandemics can cause. Since most emerging human viral diseases are zoonotic (originating from animals), predicting a pandemic at its pre-emergence stage requires a strategy to determine when a pathogen has evolved the ability to spill over from its natural animal reservoir into humans. Thus, there is a critical need to develop a comprehensive predictive framework to identify novel viral sequences with zoonotic potential and determine what genetic changes in existing viral strains could contribute to human spillover. Unfortunately, experimental approaches to identify the genetic determinants of viral shifts are expensive, low throughput, potentially dangerous, or cover only a small portion of the genome at a time, and thus incommensurate with the scale of the problem. A computational framework powered by the advances in Artificial Intelligence (AI) can address the challenge by detecting putative positive selection events in viral sequences and discriminating between sequences that infect human cells and those that do not, advancing AI for healthcare. Recently, cutting-edge advances in AI, including foundational models and large language models, have emerged as the preferred method for tackling complex sequence modeling tasks, thanks to their ability to harness large sequence databases. Yet, their reliance on expansive sequence data and parameter sets limits their flexibility and practicality in real-world scenarios such as viral zoonosis. Concurrently, the recent surge in computationally predicted protein structures by advanced deep learning models such as AlphaFold2 unlocks new opportunities in protein representation learning. While promising, the computational burden carried by such complex data still hinders practical applications in virus-host systems. To address these limitations, we propose a novel framework to enhance protein language models by integrating viral protein structural data to incorporate conformational knowledge of viral evolution. Drawing from recent advances in graph transformers, our approach will refine the self-attention mechanisms of pre-trained language transformers by integrating structural information from viral proteins with structure extractor modules. The refined foundational AI model, termed Viral Adaptation with Structure-aware Transformer (VAST), will be further pre-trained on a viral protein structure database, using the same masked language modeling objective as traditional protein language models. Using the trained model, we will seek to discover the rules of viral evolution by deriving explainable theories of mutation pathways which in turn can help us decipher the mechanisms of viral evolution and inform drug/vaccine development workflows."
NAIRR240097,1264528,2024-05-22,2024-11-22,Trustworthy QA: Enhancing Complex Reasoning in Large Language Models,"Wang, Xuan",Virginia Tech,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"Question answering (QA) aims to offer accurate answers to natural language queries based on a given context. Question answering has a wide range of downstream applications, including search engines, chatbots, and virtual assistants. The state-of-the-art QA systems are built on top of pre-trained large language models (LLMs), especially open-source LLMs that are widely adopted by companies in consideration of user data privacy. However, there is a growing concern about the accuracy and truthfulness of the information provided by these QA systems. To address this trustworthiness problem of open-source LLMs for the QA systems, we propose an open-domain QA model that 1) grounds the open-source LLMs’ (e.g., Llama) reasoning process to external evidence from knowledge bases (e.g., Wikipedia), 2) enhances the open-source LLMs’ evidence reasoning ability by analyzing the LLMs’ internal uncertainty (e.g., attention distribution), and 3) further boosts the open-source LLMs’ evidence reasoning ability with guidance from state-of-the-art black-box LLMs (e.g., GPT-4 from OpenAI). The expected outcome is an LLM with enhanced complex reasoning ability, minimizing issues like hallucination and dead-end reasoning scenarios."
NAIRR240098,1264554,2024-06-06,2024-12-06,Bayesian Nonlinear Intrinsic Dimension Learning in ImageNet,"Bhattacharya, Shrijita",Michigan State University,Statistics and Probability,NAIRR PIlot,"In statistical literature, there is a plethora of research which assumes that a high dimensional dataset (ambient space) usually sits in a lower dimensional manifold (intrinsic) such that the lower dimensional manifold can capture bulk of the statistical properties in the original high dimensional data. In this project, we intend to develop a Bayesian nonlinear approach to learn the intrinsic dimension of huge datasets like ImageNet. Nonlinear learning allows one to explore intrinsic spaces of the data which sit in a nonlinear transform of the ambient space.  The Bayesian approach provides the overall uncertainty quantification in joint estimation of the intrinsic dimension and the nonlinear map to the intrinsic space. The proposed method can be exploited to learn compressed products of large image datasets that are easier to deploy for future downstream tasks."
NAIRR240100,1263665,2024-06-06,2024-12-06,Integrating Logic Reasoning with Vision Language Models,"Kant, Krishna",Temple University,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"The goal of this project is to study Video monitoring based situation recognition in a variety of environments. In the past we have very successfully integrated explicit logic reasoning (using run-time event calculus) with traditional computer vision (TCV) for several applications. We would like to bring the emerging video language models (VLMs) into the mix not only for additional, high-level description, but also to automate the formulation of logic assertions from those descriptions with the help of TCV and compatibility/consistency across multiple versions of VLMs that are fine-tuned with correlated events. One significant application of the technique is in our ongoing NSF project to study student engagement in virtual classrooms, where we believe that VLMs could helpful in generalizing the results obtained through TCV techniques."
NAIRR240101,1264538,2024-06-06,2024-12-06,Quantification of Mask Efficacy in Large Population,"Shoele, Kourosh",Florida State University,Fluid and Plasma Physics,NAIRR PIlot,"Airborne transmission of many viruses is possible through virus-laden fluid particles evicted from the mouth of an infected person to be inhaled by a healthy person. Using a facemask with proper fitness is an effective method to diminish the airborne transmission of pathogenic agents. To protect against the virus, mask fit on the wearer's facial topology and effective mask fabric are two critical components.  Here, we leverage new machine learning algorithms along with theoretical physics-based mathematical models to form predictive metrics about mask efficacy in a very large cohort of subjects and diverse daily activities such as talking. By determining realistic measures of mask efficacy in a large diverse population, we pay the way to specify how effectively one can evaluate the practical impact of mask usage on the transmission of respiratory diseases during early stages of potential respiratory diseases."
NAIRR240112,1264570,2024-06-06,2024-12-06,Scaling Resilient and Adaptable Decentralized Learning to Large Dynamic Networks,"Inouye, David",Purdue University,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"This proposal addresses the challenge of deploying resilient  federated learning (FL) for distributed model training and inference on highly dynamic edge networks. We aim to develop methods that leverage pre-trained models to achieve robust performance under changing network conditions, including extreme device failures and adversarial attacks. Our research focuses on the vertical federated learning (VFL) setting, where edge devices contribute features of the same data points. Prior work by the authors requires training from scratch and has not been scaled beyond 49 devices. We propose to address these limitations by: (1) Investigating challenges in deploying pre-trained models (PMs) in dynamic environments with extreme faults. (2) Developing advanced methodologies for integrating PMs, including models like ResNet and efficient versions of large models. (3) Designing scalable and reliable FL methods for thousands of edge devices. This research will enable the development of robust and efficient VFL for real-world edge computing applications."
NAIRR240113,1264568,2024-04-22,2024-10-22,"Scalable Editing of Vision-Language Models, With Applications to Cyber-Physical Systems","Hegde, Chinmay",New York University,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"This NAIRR Pilot project seeks to enable significant advances in scalable model editing approaches for making multimodal AI models safe, robust, and aligned with desirable societal values. Model editing refers to the process of systematically adjusting the weights of an existing (trained) AI model so that the new model obeys certain desiderata. Most model editing approaches suffer from issues of scale (they work only for small-scale models) and extent (they are only able to change model properties to a limited degree). Our project seeks to test novel strategies that can significantly surpass such limitations."
NAIRR240115,1264573,2024-04-22,2024-10-22,Agricultural pest identification with robustness guarantees,"Ganapathysubramanian, Baskar",Iowa State University,"Agriculture, Forestry, and Fisheries",NAIRR PIlot,"This proposal requests computational resources on TACC Frontera to advance a transformative project in agriculture: developing a robust foundational vision model for pest identification (insects, weeds, and diseases). It builds on preliminary work with InsectNet, a model trained on a large dataset of insect images, to expand its capabilities significantly. The team aims to utilize an expanded dataset of approximately 100 million images and incorporate new architectures, including transformers, to enhance model accuracy, scalability, and robustness. A key innovation is the introduction of certified robustness to ensure model reliability against adversarial attacks, making it a crucial tool for improving agricultural outcomes globally. This project is supported by the USDA-NIFA and NSF-funded AI Institute for Resilient Agriculture and an NSF CPS Frontiers grant, underscoring its importance and potential impact on sustainable food production."
NAIRR240121,1264582,2024-05-22,2024-11-22,Assessing and Improving Safety and Alignment of LLMs,"Prakash, Atul",University of Michigan,Computer Science,NAIRR PIlot,"​​The rapid advancement of large language models (LLMs) has significantly impacted various real-world applications. However, recent research reveals that LLM behavior does not always align with human preference: LLM-generated responses can be misleading, inaccurate, and even harmful. To ensure that LLM behavior remains congruent with human goals, recent efforts focus on aligning LLMs to be helpful, honest, and harmless (HHH). The de facto approach to improving such alignment is fine-tuning via techniques such as Reinforcement-learning with Human Feedback (RLHF) and Direct Preference Optimization (DPO). Unfortunately, these approaches can be inefficient. Along similar veins, prior research has identified that LLMs, even those fine-tuned for human alignment, can be vulnerable to jailbreaking and can be caused to generate harmful content. Developing defense techniques, such as guarded LLMs and other multi-agent LLMs, is an active research area. But, it is poorly understood if such defense techniques are secure against adaptive attacks. 

In this proposal, we suggest research thrusts to assess LLMs, including multi-agent ones, with respect to safety against an adaptive adversary.  We also propose to explore efficiency strategies for  training safer and  better aligned models with respect to human preferences."
NAIRR240122,1264576,2024-04-22,2024-10-22,Community Runnable Earth Digital Intelligence Twin Training and Reforecasts,"Gagne, David",National Center for Atmospheric Research,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"The goal of the Community Runnable Earth Digital Intelligence Twin (CREDIT) project is to develop, train, and evaluate global and regional artificial intelligence numerical weather prediction (AI NWP) models in an open, scaleable, science-focused framework. We will investigate how choices in the data processing, architecture, loss functions, and evaluation lead to more accurate, stable, physically consistent, and valuable AI NWP models and produce large sets of reforecasts for further analysis and evaluation by the broader community."
NAIRR240129,1264497,2024-04-22,2024-10-22,Exploring Iterative Reasoning in LLMs and the Transferability to Multimodal Models,"Winder, John",Johns Hopkins University Applied Physics Laboratory,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"This project aims to explore the reasoning process of Large Language Models (LLMs) and investigate the generalizability of these capabilities from text to multimodal contexts using joint embedding models. We hypothesize that transformers, the underlying architecture of LLMs, perform semantic reasoning through an iterative numerical optimization process. Through analyzing token transformations and employing techniques like principal component analysis (PCA), we seek to understand and interpret LLMs' reasoning processes. Additionally, we will investigate the transferability of this reasoning process to other modalities by leveraging the ImageBind model. This study not only seeks to better understand the reasoning mechanisms of LLMs but also to apply these findings to modalities beyond text."
NAIRR240137,1264567,2024-06-06,2024-12-06,Evaluating and Mitigating Biases in Clinical Decision Models,"Wang, Wei",University of California Los Angeles,Health Sciences,NAIRR PIlot,"The digitization of clinical records and the improvement of large language models enables the clinical decision models to intervene the clinical operations with predicted diagnosis decisions and foreseen outcomes. The safety and fairness of the clinical decision models are crucial for developers, hospitals, clinicians and patients. We propose to conduct a comprehensive evaluation with counterfactual data generation techniques to evaluate whether the clinical decision models treat patients of different demographic groups with various behavioral patterns fairly. We further propose to amplify the hidden bias in clinical decision models with in-context influence tracing and then optimize the clinical decision models to mitigate the negative influence of these hidden biases towards downstream tasks. With our proposed method, we would obtain the first comprehensive bias evaluation results across multiple bias sources and various clinical operation components. The bias mitigation method is expected to reduce the hidden bias without the need for additional human annotations."
NAIRR240138,1264597,2024-05-22,2024-11-22,Towards Open-World Jailbreaks for Large Language Models,"Xie, Cihang",University of California Santa Cruz Jack Baskin School of Engineering,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"As the cornerstone of a new era in AI, generative AI (GenAI) such as GPT-4 promises to catalyze a profound transformation across numerous sectors of society, from education and healthcare to entertainment and commerce. These models, with their unparalleled ability to comprehend and generate multiple data types — including text, images, and audio — at a human-alike level, signify a paradigm shift, unlocking applications previously considered unattainable. However, alongside their vast potential, these models are prone to critical challenges, including the generation of erroneous or harmful content. 

This proposal zeroes in on the critical examination of the robustness of Large Language Models (LLMs). Specifically, through the novel technical designs in ensemble attacks and optimization, we aim to advance the transferability of jailbreaking LLMs, ensuring their effectiveness even in open-world setups. By proactively and adversarially identifying these vulnerabilities, we aim to understand the specific ways in which LLMs break down, providing insights that will be pivotal in steering the evolution of GenAI systems towards greater safety and reliability."
NAIRR240140,1264589,2024-04-22,2024-10-22,Developing Reliable Agents for Automating Complex Digital Tasks,"Balasubramanian, Niranjan",Stony Brook University,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"Generative AI has greatly expanded the scope of automation in complex tasks. Many day-to-day digital tasks require interacting with tools via APIs to solve problems in a step-by-step manner. For example, paying what we owe friends for a night out involves using apps such as Splitwise, emails, and Venmo. Many such common activities are complex tasks, requiring our interaction with multiple applications. If LLMs can be augmented with API-based access to these apps, we can automate such complex tasks. There have been significant advances in LLM tool use, and an increased push for automation in solving complex tasks. ChatGPT Plugins, for example, aim to support real-time information retrieval, access to specific knowledge bases, and perform actions such as booking a flight on behalf of a user. There is vast potential for solving a wide variety of complex tasks through such LLM-based automation. 

However, translating this potential into viable and verified real-world solutions requires both substantial advances in model capabilities and rigorous evaluation in a controllable environment. To this end, this team has already developed, AppWorld, an execution environment, a collection of complex tasks, and programmatic evaluation to verify the correctness of the solutions exhaustively. Benchmarking results show that standard ways of applying state-of-the-art models are unreliable and ineffective for these tasks. This proposal is aimed at developing and training reliable LLM-based agents using three approaches: (i) Supervising LLMs with iterative bootstrapping of tasks, (ii) Learning with feedback from the programmatic evaluator, and (iii) Learning via self-exploration within the execution environment."
NAIRR240141,1264598,2024-06-06,2024-12-06,TEA: Two-Player LLM Games with an LLM Referee,"Zampetakis, Manolis",Yale University,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"Our project focuses on Large Language Models (LLMs), which are a rapidly advancing model of Artificial Intelligence. Our goal in this project, at a high level, is to design methods to increase the “performance” of an LLM, reduce its susceptibility to adversarial attacks, and also introduce metrics to measure its performance. In more detail, we aim to measure and enhance the performance of a “Target LLM” through its interactions with two other LLMs. One of them, the “Attacker LLM”, has a competing objective with the target, and the other one, the “Evaluator LLM”, evaluates the performance of both the Target and the Attacker and provides feedback. Our setup is designed so that the test of the performance or the improvement of the target LLM happens automatically without any human supervision. In particular, we plan to consider the interaction between the Target, the Evaluator, and the Attacker (TEA) in a game-theoretic setup."
NAIRR240143,1264605,2024-06-06,2024-12-06,NAIRR Pilot: Computational Foundations for Tractable Deep Generative Models,"Van den Broeck, Guy",University of California,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"Probabilistic circuits represent joint distributions as deep computation graphs. They move beyond other deep generative models and probabilistic graphical models by guaranteeing tractable probabilistic inference for certain classes of queries: marginal probabilities, entropies, expectations, causal effects, etc., can all be computed exactly and efficiently, often in linear time. As such, probabilistic circuits provide an alternative, more capable and trustworthy architecture for the future of generative AI.
Recent developments have ensured that probabilistic circuits are now also effectively learned from data at scale. Moreover, tractable deep generative models are used to control the behavior of intractable deep generative models such as transformers and diffusion models. This way, probabilistic circuits achieve state-of-the-art results in constrained sampling from large language models, natural image distributions, and offline reinforcement learning models.Nevertheless, the computational investments made so far to develop tractable deep generative models is negligible compared to more traditional architectures.To bring the dream of tractable deep generative models to fruition, we propose this pilot project which will build a solid foundation by scaling up tractable model learning and its applications."
NAIRR240144,1264601,2024-04-22,2024-10-22,"An Empirical Evaluation of Accuracy, Robustness and Bias of Compressed Language Models","Srikumar, Vivek",University of Utah,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"It is not sufficient to ensure the safety, security and trustworthiness of today's largest AI systems. Indeed, given their resource constraints, compressed versions of AI systems will be deployed more often and ensuring their safety and robustness is important. To be effective, model compression methods must maintain predictive accuracy, while ensuring out-of-domain robustness. Furthermore, the resulting models should not show disparate predictive performance across societal groups. As a first step towards the safety of compressed models, our objective is to conduct the first comprehensive, multi-dimensional evaluation of compression techniques, focusing on accuracy, reliability, robustness, and bias of the resulting models."
NAIRR240148,1263712,2024-05-22,2024-11-22,Accelerating Fusion Energy Research with Physics-Guided Deep Learning,"Yu, Rose","University of California, San Diego",Fluid and Plasma Physics,NAIRR PIlot,"In order to find the global optimum design of future fusion reactors, it is important to develop fast and accurate models of the heat and particle losses due to turbulence in magnetically confined plasmas. To accelerate the training and validation of physics-guided deep learning models trained on multi-fidelity simulation data of turbulent heat and particle fluxes, we propose using the ALCF AI Testbed's dedicated machine learning platforms. Our project will benefit tremendously from SambaNova's advanced AI-focused hardware. This platform, with its unique dataflow architecture and substantial memory capacity, promises to significantly reduce our model training times and accelerate our adaptive learning workflows for the design of a fusion pilot plant."
NAIRR240152,1264521,2024-05-22,2024-11-22,Safeguarding Generative AI by Audio-Visual Deepfake Detection,"Duan, Zhiyao",University of Rochester,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"The primary goal of this project is to advance audio-visual deepfake detection techniques to protect against the malicious use of generative AI technologies effectively. By identifying deepfakes in widely disseminated videos, we aim to curb the spread of misinformation. Our project is structured around three objectives: 1) Enhancing current detection methods with innovative approaches, 2) Extending detection capabilities to a wider array of deepfakes including those with partial audio and/or visual modifications, and 3) Improving the generalization of deepfake detection across a broader spectrum of generated content beyond mere biometric falsifications."
NAIRR240154,1264614,2024-04-22,2024-10-22,Georgetown University Advanced Computing Resource Request for AI Education,"Hickman, James",Georgetown University,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"This proposal aims to acquire computational resources for student-led projects to bolster AI education, with the goal of facilitating workforce development in the Artificial intelligence space. The Department of Data Science and Analytics (DSAN) at Georgetown University boasts a sizable and diverse population of over 200 Master's level students, with an annual cohort size of approximately 120 students. Our goal is to secure computational resources that will enable our student body to delve into a wide array of issues at the intersection of data science, deep learning, and AI. This includes tasks such as testing, evaluating, verifying, and validating AI systems and models to ensure safe, secure and trustworthy AI. Our program offers a comprehensive range of AI and deep learning courses, including DSAN-6600: Introduction to deep learning, DSAN-6500: Image Mining and Computer Vision Analytics, DSAN-6650: Deep Reinforcement Learning, DSAN-5400: Computational Linguistics, DSAN 5800: Advanced Natural Language Processing, and DSAN-5810: NLP with Large Language Models. While these courses provide our students with a robust skill set in deep learning, their projects are currently significantly restricted by our limited access to GPU and super-computer resources. Therefore, our objective in submitting this proposal is to enhance our department's computational resources, thereby expanding the complexity of the models and methodologies accessible to our students."
NAIRR240156,1264603,2024-06-06,2024-12-06,Trustworthy Embodied 3D Vision-Language Question Answering with Situational Encoding,"Wang, Yu-Xiong","Department of Computer Science, University of Illinois Urbana-Champaign",Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"Achieving the integration of multi-modal information and logical reasoning represents a significant milestone in developing conversational and human-centered embodied AI. Although recent research has demonstrated considerable progress in linguistic understanding and image comprehension using natural language commands or dialogues, the ability to perceive and reason in real-world 3D embodied environments still falls short of human expectations, due to limitations in 3D reasoning capabilities and interpretability. This project aims to explore 3D embodied scene comprehension, wherein an agent is required to understand its situation and surroundings within a dynamic, egocentric environment and subsequently perceive, reason, answer, and act accordingly to complete complex tasks."
NAIRR240158,1264617,2024-06-06,2024-12-06,Towards Understanding and Improving Multilingual Abilities of Large Language Models,"Agrawal, Ameeta",Portland State University,Computer Science,NAIRR PIlot,Multilingual LLMs
NAIRR240160,1264619,2024-06-06,2024-12-06,GraphNarrator: Natural Language Generation From Large-Scale Open- Domain Knowledge Graph,"Li, Chengkai",The University of Texas at Arlington,Artificial Intelligence and Intelligent Systems,NAIRR PIlot,"The GraphNarrator project, led by Dr. Chengkai Li and his team, focuses on generating natural language descriptions for fragments of large-scale knowledge graphs. These descriptions aim to help users understand complex graph data more easily. The project trains narration models using pre-trained language models like BART, T5, and LLaMA on a dataset comprising 8.7 million pairs of Freebase graph fragments and corresponding Wikipedia sentences. The key challenge addressed is the problem of information hallucination in existing graph-to-text models, where generated text may contain fabricated facts not present in the input graph. The project proposes a novel approach to mitigate hallucination by trimming generated text using dependency parse trees to eliminate portions not represented in the input graph. This approach aims to bridge the gap between graph-to-text models and real-world applications, enhancing the reliability of generated descriptions."
NAIRR240161,1264616,2024-04-22,2024-10-22,LakeGPT: Building a Foundation Model for Aquatic Sciences,"Karpatne, Anuj",Virginia Tech,Ecology,NAIRR PIlot,"The goal of this project is to build a foundation model for aquatic sciences, termed Lake-GPT, to model a variety of processes related to the quality of water in lakes and reservoirs, using novel advances in the emerging field of ecology knowledge-guided machine learning (eco-KGML). Supported by an NSF grant from the Macrosystems Biology and NEON-Enabled Science (MSB-NES) program, our work is opening a new chapter of research in AI to model scientific systems where we have growing volumes of data collected from sensors as well as generated from model simulations. Our research in eco-KGML also makes a distinct departure from mainstream practices of building “black-box” AI models, that solely rely on supervision contained in data, to also leverage the wealth of scientific knowledge available in many domains including aquatic sciences (e.g., conservation laws of mass and energy) in diverse formats (as equations, rules, heuristics, or as mechanistic model runs) and with varying fidelities of knowledge (ranging from perfect and complete to imperfect and partial). Inspired by the success of large-scale foundation models in mainstream applications of computer vision and language understanding in the commercial arena, the domain of aquatic sciences is ripe with opportunities to investigate the effectiveness of foundation models for modeling scientific processes using both scientific knowledge and data."
NAIRR240165,1264551,2024-06-06,2024-12-06,Towards Trustworthy Biomedical Models via Robust Imbalance Learning,"Huang, Xiaolei",The University of Memphis,"Informatics, Analytics and Information Science",NAIRR PIlot,"The project aims to enhance the robustness of biomedical Large Language Models (LLMs) against data imbalances in health datasets. Recognizing the criticality of addressing skewed disease categories, demographic sizes, and symptom frequency, this research focuses on improving model reliability across diverse and uncertain environments. The real examples' shortcomings in adapting to new data distributions underscores the urgency for developing models resilient to imbalance challenges, particularly those arising from heterogeneous sources like clinical records and social media.

With a six-month timeline, the project intends to scrutinize LLMs' robustness against various imbalance scenarios and devise methods to bolster model robustness through novel domain adaptation techniques. The research will leverage the expertise of the lead PI, Dr. Xiaolei Huang, in NLP and health informatics, and Co-PI Dr. I-Chan Huang’s background in epidemiology and cancer control, with a focus on pediatric oncology at St. Jude Children’s Research Hospital. The project's phased approach includes an initial examination of imbalance effects on model performance and fairness, followed by the development and evaluation of imbalance adaptation methods.

The team is prepared to promptly utilize NAIRR Pilot Project resources upon access, with all members compliant with necessary regulations. The project's commitment to open-source practices and educational integration promises widespread benefits to the research community, reinforcing its potential for significant contributions to the field of computational healthcare."
NAIRR240167,1268520,2024-06-03,2024-12-03,Bubble Aid: Assistive AI to Improve the Robustness and Security of Reading Hand-Marked Ballots,"zhang, chengcui",The University of Alabama at Birmingham,Computer Science,CloudBank Pilot,This project develops an AI-based program to improve the efficiency and security and hence trustworthiness of election systems that use hand-marked ballots.
NAIRR240168,1270428,2024-06-03,2024-12-03,Data-driven Formal Approach to Safe Autonomy,"Fan, Chuchu",MIT,Artificial Intelligence and Intelligent Systems,CloudBank Pilot,"This work supports the computational resource for the project data-driven formal approach to safe autonomy, which investigates the theoretical and algorithmic foundations of the methods for the control, design, and risk analysis of highly challenging autonomous systems."
NAIRR240169,1271461,2024-06-03,2024-12-03,Networking and Compute for Next Generation Low-Earth Orbit Satellites,"Vasisht, Deepak",University of Illinois at Urbana-Champaign,Computer Science,CloudBank Pilot,"The next generation of Low Earth Orbit (LEO) satellites, dedicated to Earth observation, seeks to enable a compelling vision: frequent high-resolution monitoring of the Earth to track humanity scale events. These satellites operate in low orbits, less than 1000 Km above Earth, and democratize access to Earth imagery for multiple applications: precision agriculture that improves farm yields and incomes, disaster management such as early detection of forest fires, modeling the spread of diseases, geo-political analytics, and climate monitoring. As such constellations scale to hundreds of satellites, traditional architectures for space-Earth connectivity fail to meet their needs. Specifically, these constellations generate hundreds of terabytes of data every day that must be transported to the Earth, while the satellites travel at fast speeds with respect to ground stations on the Earth. Therefore, the data download process experiences day-level delays and is prone to failures due to bad weather or hardware errors. The goal of this proposal is to design a new paradigm for space-Earth connectivity that removes these bottlenecks and enables fault-tolerant networks that generate near-realtime (minute-level) insights from data collected by these satellites. The proposed research will remove networking and compute bottlenecks for LEO satellites and enables novel commercial, academic, and national security applications. Some example applications include early response to aircraft intrusions, forest fires, and geopolitical events. 

The proposed research designs new algorithms and architectures for networking and edge computing on satellites and ground stations. Specifically, the proposed research includes: (a) a new distributed ground station architecture that is robust to transient failures, agile to traffic variations, and enables low latency data download from satellites, (b) an edge computing framework that extracts insights, schedules data transfer, and prioritizes important imagery for networked transfer while being scalable to multiple applications, and (c) design and analysis of interconnects between communication and Earth observation satellites for opportunistic routing. The proposal takes an end-to-end systems level approach to large-scale space-Earth networks, which is essential for their robust, fault-tolerant, and high-performance operation."
NAIRR240170,1269782,2024-06-03,2024-12-03,Building Trustworthy Multimodal Models for Oncology Applications,"Rasool, Ghulam",Moffitt Cancer Center,Artificial Intelligence and Intelligent Systems,CloudBank Pilot,"Trustworthiness, safety, and reliability of AI systems are fundamental requirements for any meaningful application of these systems in healthcare. However, developing such models using multimodal data modalities such as radiological images, multi-omics data, pathology images, and clinical notes is challenging. This project explores an innovative approach centered on building embeddings using a deep Bayesian approach for diverse modalities, facilitating the creation of scalable and trustworthy multimodal machine learning models. By harnessing the power of embeddings and Bayesian deep learning, our methodology seeks to effectively fuse information from disparate sources, enabling a holistic understanding of the patient's condition in a trustworthy and safe way. By developing scalable models based on the Bayesian Transformer architecture, our approach aims to enhance the accuracy, efficiency, and trustworthiness of oncological diagnoses and treatment strategies, ultimately contributing to improved patient outcomes."
NAIRR240171,1271826,2024-06-03,2024-12-03,Efficient approaches for specializing large language models for domain specific text,"Sontag, David",MIT,Computer Science,CloudBank Pilot,"Our team is planning to apply the latest large language models to analyze domain specific-text, e.g., clinical notes. Recently, there are various new approaches to efficiently fine-tune the models for domain specific text, ranging from LoRA (training small “modifier” weights based on low rank adaptation) to model merging (merging parameters between models for enhanced performance). We aim to explore other machine learning approaches for effective adaptation/specialization of the models. One example is our recent work, Co-LLM, that trains a base LLM to collaborate with other more powerful assistant LLMs. We plan to extend our work and investigate other efficient approaches for fine-tuning LLMs on domain-specific text."
NAIRR240172,1271916,2024-06-03,2024-12-03,Privacy Preserving Tutoring System for Health Education of Low Literacy Hispanic Populations,"Cao, Zechun",Texas A&M University - San Antonio,Computer Science,CloudBank Pilot,"This project will implement a computer tutor for low literacy Hispanic breast cancer survivors. Breast cancer is the leading cause of cancer-related deaths in Hispanics, and although research has shown that education can greatly mitigate stress and improve quality of life, few educational interventions for this population exist. The computer tutor resulting from this project will mimic a human tutor that teaches about breast cancer survivorship skills and about breast cancer in general. Because tutoring involves conversation with the survivor, it is possible that they reveal sensitive personal information; therefore, it is important to encrypt the information in the tutoring session to prevent any privacy breaches. And for the tutoring component to be effective, special attention must be paid to the utilization of natural language processing models as well as models of behavior that are observed in the target population when tutoring and/or interacting with technology. For the privacy component to be effective, techniques that can encrypt and decrypt data at high speeds will be explored to make the interaction fluid. To date, computer tutors that converse with their students have been tried mainly with a highly literate population in college settings, so their impact on low literacy Hispanics is unknown. Therefore, this project will advance our understanding of the impact of designing artificial intelligence powered tutors to address diversity and disparities in the access to information by a subset of low literacy individuals, as well as our understanding of privacy preserving algorithms that work in real-time with complex natural language processing models. More broadly, project outcomes will facilitate access to information for minority populations and will serve to build research capacity and train minority students in the participating teaching-oriented institutions.

The project will be carried out with two objectives in mind. First, development of a novel intelligent computer tutoring system that is customized so that it can effectively query and interact with Hispanic breast cancer survivors by adapting existing content that was created for this population in prior research. Because it has been shown that both the language of many adult Hispanics, and target population interactions with technology, are more nuanced than previously thought, our first objective also involves training natural language algorithms and designing interactions that model those of Hispanic breast cancer survivors. The second objective is to develop privacy-preserving algorithms that utilize robust end-to-end encrypted communication and can encrypt and decrypt distributed data in real time at a speed that does not hinder the interactions with the computer tutor. The contributions of this development process will be threefold: (1) to understand the role of culture and education in the interaction between low literacy Hispanic breast cancer survivors and intelligent tutoring systems; (2) to develop a framework that facilitates the implementation of intelligent tutoring systems for minority populations; and (3) to develop accurate and low latency privacy preserving mechanisms for NLP model training and dialogue interfaces."
NAIRR240173,1272046,2024-06-03,2024-12-03,REU Site: Engineers for Exploration,"Kastner, Ryan",University of California San Diego,Applied Computer Science,CloudBank Pilot,"Engineers for Exploration (E4E) is a one-of-a-kind program centered around multidisciplinary and collaborative student research projects with the broad goals of protecting the environment, uncovering mysteries related to cultural heritage, and providing experiential learning experiences for undergraduate and graduate students. We team student engineers with scientists from a wide range of disciplines, such as ecology, oceanography, and archaeology. Students create new technologies to aid these scientists in their work and then accompany them on field deployments around the world. Projects train students in embedded systems and software, machine learning, electronic integration, mechanical design, system building, as well as project management and team leadership."
NAIRR240174,1273069,2024-06-03,2024-12-03,"Large spatial network generation, transformation, and interpretation","Zhao, Liang",Emory University,Computer Science,CloudBank Pilot,"Spatial networks are complex networks whose nodes and edges are embedded in space. Spatial network data has been experiencing a fast increase recently, ranging from micro-scale (e.g., protein structure) to middle-scale (e.g., biological neuron network) to macro-scale (e.g., mobility networks), where the spatial and network topology jointly determines the network formation, evolution, and function. Modeling and understanding the generation and transformation of spatial networks are long-standing and challenging research domains crucial to broad applications such as medicine signs, mental disease early diagnosis, and societal event synthesis, whose underlying principles are largely unknown yet. On the one hand, existing spatial network generative modeling methods heavily rely on human-predefined principles but cannot automatically learn new ones. On the other hand, deep generative models can learn new underlying principles from the data but are not good at incorporating external principles, which is indispensable to ensure valid space and network topology of the generated spatial networks. Motivated by the above complementary strengths and drawbacks, this project aims at a new framework that can: 1) automatically learn new generation and transformation processes of spatial networks, 2) embed user-specified principles to constrain and regularize the generated spatial networks, and 3) pursue the model interpretability and automatically distill new understandable principles of network process. The research approaches include the development of: 1) Novel spatial and spectral graph decoders for large spatial networks, 2) Deep generative modeling and optimization with spatial and topological regularization, 3) A variety of novel spatial- and spectral-graph transformation strategies, and 4) A novel system for interacting predefined and distilled principles between human and models."
NAIRR240175,1273205,2024-06-03,2024-12-03,Cloud Computing Credits for Studying Deep Generative Models,"Fan, Liyue",UNC Charlotte,Computer Science,CloudBank Pilot,The objective of this project is to design an intelligent deepfake detector that will be capable of assessing the integrity of digital visual content and automatically detect falsified images or videos.  The success of the proposed research will benefit our society by providing a more trustworthy and healthy environment for billions of social network users and ensuring the authenticity of visual content for digital forensics.
NAIRR240176,1272310,2024-06-03,2024-12-03,Socially-Aware Language Technologies To Support People in Supporting Others for Better Online Communities,"Yang, Diyi",Stanford University,Artificial Intelligence and Intelligent Systems,CloudBank Pilot,"My research goal is to support people in better supporting others and  accomplishing their tasks by providing them with assistance through building scalable socially aware language technologies, with a special focus on online peer support groups.  How do people support others in need in online support groups?  How can we build social computing systems to support people who are supporting others? Supporters are the key to the success of online peer support groups, which are used by millions of people with health concerns. However, online supporters often do not receive rigorous training and tailored feedback, which might lead to unsupportive or even negative helping behaviors.  Existing mechanisms of training or scaffolding largely rely on human supervision,  making it hard to scale up to help the large number of supporters who  support millions of people in need of care.  This work will fill these gaps and accomplish the vision of supporting people in better supporting others  in several representative text-based online peer support groups  by: (1) developing innovative natural language processing techniques to predict supporters' helping skills and examining how helping skills related to positive outcomes; (2) designing contextualized language generation approaches that provide tailored assistance for supporters by highlighting which helping skills are needed in a given situation and suggesting example responses with actionable feedback; 
and (3) creating an open-source and human-in-the-loop tool to empower supporters and evaluating how the tool can be used for both training and real-time scaffolding via lab studies, field experiments, and real-world deployment."
NAIRR240177,1271454,2024-06-03,2024-12-03,Advancing the Accessibility of STEM Online Learning by Using Generative AI,"Huang, Yun",UIUC,Visualization and Human-Computer Systems,CloudBank Pilot,"Videos are a popular medium for online learning, but research has found that accessing these materials is challenging for Deaf and Hard-of-Hearing (DHH) learners. This project aims to develop and evaluate effective Q/A mechanisms and interaction designs that enable both DHH and hearing students to collaboratively access STEM videos. Specifically, we propose creating deep learning models capable of generating questions that are relevant and inspiring, thus prompting students' curiosity and encouraging them to explore the learning materials more deeply. Our project will be conducted in two phases: the development of video question generation models and pipelines, followed by a large-scale user study to evaluate their effectiveness in a real-world setting. The team boasts expertise in Human-Computer Interaction, Natural Language Processing, and AI-augmented learning technologies. Evaluation sites include Gallaudet University, the world’s only liberal arts institution dedicated exclusively to the education of Deaf and Hard-of-Hearing learners, and the University of Illinois at Urbana-Champaign, which is committed to fostering an inclusive learning environment."
